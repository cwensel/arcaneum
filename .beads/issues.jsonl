{"id":"arcaneum-1","title":"Create Claude Code marketplace project structure","description":"Create the foundational directory structure and packaging setup for the Arcaneum Claude Code marketplace project. This establishes where all code lives and how components are packaged/distributed.\n\nDeliverables:\n- pyproject.toml with modern Python packaging\n- Directory structure: src/, plugins/, docs/, tests/\n- MCP server plugin scaffold\n- Installation method via uvx or pip\n- README with quick start guide\n\nThis is the foundation that all other RDRs build upon.","design":"Directory Structure:\n```\narcaneum/\n├── pyproject.toml          # Modern packaging\n├── README.md               # Quick start\n├── src/\n│   └── arcaneum/\n│       ├── __init__.py\n│       ├── server/         # MCP server core\n│       ├── indexing/       # Bulk upload logic\n│       └── search/         # Search utilities\n├── plugins/\n│   ├── qdrant-server/      # Server management plugin\n│   ├── qdrant-indexer/     # Bulk upload plugin\n│   └── qdrant-search/      # Search plugin\n├── tests/\n└── docs/\n\nInstallation:\n- uvx install arcaneum\n- pip install arcaneum\n\nMCP Plugin Discovery:\n- Each plugin has its own pyproject.toml\n- Plugins register via entry points\n- Claude Code discovers via marketplace registry","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.685704-07:00","updated_at":"2025-10-20T07:41:10.685704-07:00","closed_at":"2025-10-19T10:23:12.252341-07:00"}
{"id":"arcaneum-10","title":"Review Chroma embedding references and validate for Qdrant","description":"Review chroma-embedded/upload.sh and outstar-rag-requirements.md for embedding model usage patterns. Validate if these approaches apply to Qdrant or need adaptation.","notes":"Review completed. ChromaDB patterns transfer directly to Qdrant with adaptations: increase batch size to 100-200, client-side embeddings, same chunking strategies. FastEmbed equivalence needs validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.68751-07:00","updated_at":"2025-10-20T07:41:10.68751-07:00","closed_at":"2025-10-19T14:30:51.356705-07:00","dependencies":[{"issue_id":"arcaneum-10","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.722364-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-11","title":"Research Qdrant client-side embedding capabilities","description":"Review qdrant-client Python library source code for embedding model integration, FastEmbed support, and collection configuration options. Document features and APIs.","notes":"Research completed. Qdrant client has deep FastEmbed integration. Collection creation via create_collection() with VectorParams. Named vectors supported. HNSW config: m=16, ef_construct=100 defaults.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.688474-07:00","updated_at":"2025-10-20T07:41:10.688474-07:00","closed_at":"2025-10-19T14:30:51.424705-07:00","dependencies":[{"issue_id":"arcaneum-11","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.723279-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-12","title":"Research opensource projects for collection management","description":"Search for existing opensource tools/libraries that manage Qdrant collections with embedding models. Evaluate as replacement or inspiration (mcp-server-qdrant, qdrant-haystack, etc).","notes":"Research completed. Recommendation: Build from scratch using qdrant-client+FastEmbed (Apache 2.0). Avoid GPL-3.0 tools. Inspiration from analogrithems/qdrant-cli structure, mcp-server-qdrant patterns. Bundle QdrantUI as companion.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.689348-07:00","updated_at":"2025-10-20T07:41:10.689348-07:00","closed_at":"2025-10-19T14:30:51.484132-07:00","dependencies":[{"issue_id":"arcaneum-12","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.724072-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-13","title":"Design init vs create-collection CLI architecture","description":"Design CLI structure with init (one-time setup) and create-collection commands. Determine what belongs in init (model cache setup, server validation) vs collection creation.","notes":"Design completed. CLI structure: init (server health check, model cache setup, server config validation) and collection commands (create, list, delete, info). Collection creation includes named vectors setup, HNSW config, payload indexes. Model management: list, download, cache-info commands.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.690311-07:00","updated_at":"2025-10-20T07:41:10.690311-07:00","closed_at":"2025-10-19T14:31:31.235733-07:00","dependencies":[{"issue_id":"arcaneum-13","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.724801-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-14","title":"Write RDR-003 document","description":"Create doc/rdr/RDR-003-collection-creation.md with all research findings, design decisions, and implementation guidance.","notes":"RDR-003 document written with all sections complete. Includes metadata, problem statement, research findings, technical design, CLI architecture, configuration system, implementation examples, alternatives, trade-offs, implementation plan, validation, and references.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.691206-07:00","updated_at":"2025-10-20T07:41:10.691206-07:00","closed_at":"2025-10-19T14:42:26.150428-07:00","dependencies":[{"issue_id":"arcaneum-14","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.725549-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-15","title":"Review prior RDRs for context and design patterns","description":"Read all existing RDR files in doc/rdr/ to understand established patterns, design decisions, and technical approaches that should influence the PDF indexing RDR.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.692233-07:00","updated_at":"2025-10-20T07:41:10.692233-07:00","closed_at":"2025-10-19T17:01:39.984235-07:00"}
{"id":"arcaneum-16","title":"Review chroma-embedded/upload.sh for PDF processing patterns","description":"Analyze the referenced chroma-embedded/upload.sh script, specifically lines 1372-1522 (PDF extraction) and lines 269-324 (token-optimized chunking) to understand existing implementation patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.69306-07:00","updated_at":"2025-10-20T07:41:10.69306-07:00","closed_at":"2025-10-19T17:03:49.096247-07:00"}
{"id":"arcaneum-17","title":"Review outstar-rag-requirements.md for PDF requirements","description":"Read outstar-rag-requirements.md lines 136-167 to understand the specific PDF processing requirements for this project.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.693879-07:00","updated_at":"2025-10-20T07:41:10.693879-07:00","closed_at":"2025-10-19T17:04:23.029999-07:00"}
{"id":"arcaneum-18","title":"Research PyMuPDF (fitz) capabilities and limitations","description":"Deep dive into PyMuPDF open source code to understand text extraction capabilities, table handling, performance characteristics, and edge cases.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.694701-07:00","updated_at":"2025-10-20T07:41:10.694701-07:00","closed_at":"2025-10-19T17:10:46.570322-07:00"}
{"id":"arcaneum-19","title":"Research pdfplumber capabilities and limitations","description":"Deep dive into pdfplumber open source code to understand text extraction capabilities, table handling, performance characteristics, and when it excels vs PyMuPDF.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.695536-07:00","updated_at":"2025-10-20T07:41:10.695536-07:00","closed_at":"2025-10-19T17:10:46.614683-07:00"}
{"id":"arcaneum-2","title":"RDR for running Qdrant server with embedding model configuration","description":"Create an RDR that defines how to run and configure a Qdrant server instance for the Arcaneum marketplace. Must address Docker vs local binary, port configuration, volume persistence, and embedding model setup.\n\nKey Design Questions:\n- Docker official image or custom build?\n- How to configure FastEmbed for client-side embeddings?\n- Volume mounting strategy for data persistence\n- Multi-model support per collection\n- gRPC vs REST API preference\n\nReferences:\n- /Users/cwensel/sandbox/outstar/research/qdrant-local/server.sh\n- outstar-rag-requirements.md sections on Qdrant (lines 82-94)","design":"Initial Design Direction (to be refined in RDR):\n\nDocker Setup:\n- Official qdrant/qdrant:latest image\n- Port 6333 (REST), 6334 (gRPC)\n- Volume: ./qdrant_storage:/qdrant/storage\n- Health checks via /health endpoint\n\nEmbedding Strategy:\n- Client-side with FastEmbed (no server modification)\n- Models: stella (1024d), modernbert (1024d), bge-large (1024d), jina-code (768d)\n- Model cache: ./models_cache/ mounted\n\nCollection Architecture:\n- One collection per (document-type, embedding-model) pair\n- Example: outstar-source-code-jinacode, outstar-pdf-stella\n- Metadata stores embedding model name for validation\n\nServer Management:\n- Start/stop/restart commands\n- Log access\n- Resource limits (4GB RAM default)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.696384-07:00","updated_at":"2025-10-20T07:41:10.696384-07:00","closed_at":"2025-10-19T10:42:09.54722-07:00","dependencies":[{"issue_id":"arcaneum-2","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.726613-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-20","title":"Research Tesseract OCR capabilities and integration","description":"Investigate Tesseract OCR system dependencies, language support, accuracy characteristics, confidence scoring, and Python integration patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.697254-07:00","updated_at":"2025-10-20T07:41:10.697254-07:00","closed_at":"2025-10-19T17:10:46.656407-07:00"}
{"id":"arcaneum-21","title":"Research EasyOCR capabilities and integration","description":"Investigate EasyOCR pure Python implementation, language support, accuracy vs Tesseract, performance characteristics, and integration patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.698059-07:00","updated_at":"2025-10-20T07:41:10.698059-07:00","closed_at":"2025-10-19T17:10:46.702429-07:00"}
{"id":"arcaneum-22","title":"Research embedding model token limits and chunking strategies","description":"Investigate token limits for stella, modernbert, and bge-large models. Understand optimal chunking strategies, overlap recommendations, and char-to-token ratios.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.699076-07:00","updated_at":"2025-10-20T07:41:10.699076-07:00","closed_at":"2025-10-19T17:16:49.241549-07:00"}
{"id":"arcaneum-23","title":"Research Qdrant batch upload capabilities and best practices","description":"Investigate Qdrant's batch upload API, optimal batch sizes, error handling, retry strategies, and performance characteristics for large-scale indexing.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.699932-07:00","updated_at":"2025-10-20T07:41:10.699932-07:00","closed_at":"2025-10-19T17:16:49.284168-07:00"}
{"id":"arcaneum-24","title":"Research chroma-embedded git handling patterns","description":"Analyze chroma-embedded/upload.sh git handling (lines 373-433, 846-976) to understand:\n- Git project discovery with find .git and depth control\n- Metadata extraction (commit_hash, remote_url, branch, project_name)\n- Change detection logic comparing stored vs current commit\n- Bulk deletion of changed projects\n- Integration with .gitignore via 'git ls-files'\n\nReference: /Users/cwensel/sandbox/outstar/research/chroma-embedded/upload.sh","notes":"Research Complete: Analyzed chroma-embedded/upload.sh git handling (lines 373-433, 846-976).\n\nKey Findings:\n- Git discovery uses find .git with depth control (depth+1 for maxdepth calculation)\n- Metadata extraction: commit_hash (git rev-parse HEAD), remote_url (git remote get-url origin), branch (git branch --show-current), project_name (basename)\n- Change detection: Compare stored vs current commit hash, trigger bulk deletion if changed\n- Bulk deletion strategy: 2-tier batching (1000 retrieve, 100 delete) with offset pagination\n- .gitignore integration: Uses 'git ls-files' to respect ignore patterns, converts relative to absolute paths\n- Edge cases handled: detached HEAD (fallback to \"unknown\"), missing remote (fallback), shallow clones\n\nPatterns for Qdrant adaptation:\n- Use same git discovery logic\n- Implement filter-based deletion (faster than ChromaDB's ID-based)\n- Maintain commit hash comparison for change detection\n- Support depth control via CLI parameter","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.700839-07:00","updated_at":"2025-10-20T07:41:10.700839-07:00","closed_at":"2025-10-19T21:14:08.502224-07:00"}
{"id":"arcaneum-25","title":"Research ASTChunk library for multi-language code chunking","description":"Deep research on ASTChunk library capabilities:\n- Verify support for 15+ languages (Python, Java, JS/TS, C#, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift)\n- AST-aware chunking strategies preserving function/class boundaries\n- Token sizing and safety buffers\n- Fallback mechanisms when AST parsing fails\n- Integration patterns and configuration options\n\nUse opensource-code-explorer agent to find real-world usage examples.","notes":"Research Complete: Deep analysis of ASTChunk library and alternatives.\n\nKey Findings:\n- ASTChunk supports ONLY 4 languages (Python, Java, C#, TypeScript) - insufficient for requirements\n- Missing 11 required languages: JavaScript, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift\n- No built-in fallback when AST parsing fails\n- Uses cAST algorithm: recursive node splitting + greedy merging\n- Token sizing via non-whitespace character count (not actual tokens)\n\nCRITICAL DECISION: Cannot use ASTChunk alone\n\nRecommended Alternative: tree-sitter-language-pack\n- Supports 165+ languages (covers all 15+ requirements)\n- LlamaIndex CodeSplitter provides mature implementation\n- Built-in error handling and fallbacks\n- Integration pattern: get_parser(language) → AST-based chunking → fallback to line-based\n\nReal-world validation:\n- ChunkHound uses cAST with 24 languages\n- CodeSearchNet uses tree-sitter for 6 languages\n- Code-splitter (Rust) supports all tree-sitter languages\n\nImplementation: Use tree-sitter-language-pack as foundation, not ASTChunk","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.701685-07:00","updated_at":"2025-10-20T07:41:10.701685-07:00","closed_at":"2025-10-19T21:14:08.566659-07:00"}
{"id":"arcaneum-26","title":"Study qdrant-local source code handling vs chroma-embedded","description":"Compare qdrant-local and chroma-embedded approaches to source code indexing:\n- Identify Qdrant-specific optimizations vs ChromaDB patterns\n- Metadata schema differences\n- Batch upload strategies for code vs PDFs\n- Performance characteristics\n\nReference: /Users/cwensel/sandbox/outstar/research/qdrant-local/","notes":"Research Complete: Comprehensive comparison of Qdrant-local vs chroma-embedded.\n\nKey Differences:\n- Embedding: Qdrant uses client-side (FastEmbed), ChromaDB uses server-side\n- Batch sizes: Qdrant handles 100-200 chunks, ChromaDB limited to 25-50 (HTTP payload limits)\n- Deletion: Qdrant filter-based (40-100x faster), ChromaDB requires ID retrieval first\n- Communication: Qdrant supports gRPC + REST, ChromaDB HTTP only\n\nSource Code Indexing Specifics:\n- Both reduce chunk size by 60 tokens for code (better AST parsing)\n- Both reduce overlap to 50% for code\n- Same git awareness patterns (ls-files, commit tracking)\n- Same AST chunking integration (when available)\n\nQdrant Optimizations to Leverage:\n1. Use filter-based deletion for bulk operations (50-500ms vs 20-50s)\n2. Increase batch sizes to 100-200 (no HTTP limits)\n3. Use gRPC for 2-3x faster uploads\n4. Hierarchical metadata grouping for efficient filtering\n5. Native metadata filtering (no client-side deduplication needed)\n\nPerformance: Qdrant's client-side embeddings + native filtering make it superior for complex multi-project indexing workflows.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.7024-07:00","updated_at":"2025-10-20T07:41:10.7024-07:00","closed_at":"2025-10-19T21:14:08.633835-07:00"}
{"id":"arcaneum-27","title":"Research jina-code embedding model characteristics","description":"Investigate jina-code (jina-embeddings-v3) for code embeddings:\n- Token limits and context window\n- Optimal chunk sizes for code (400 tokens target?)\n- Character-to-token ratios for various programming languages\n- Code-specific optimizations vs general embeddings\n- Comparison with stella/modernbert for code workloads\n\nUse web search and opensource research for jina-code documentation.","notes":"Research Complete: Jina embedding models for code analyzed.\n\nKey Findings:\nTHREE Jina models for code (not just one):\n1. jina-embeddings-v2-base-code: 161M params, 8K context, 768D, 0.7753 accuracy, Apache 2.0\n2. jina-embeddings-v3: 570M params, 8K context, 1024D, 0.7564 accuracy (WORSE for code), CC BY-NC 4.0\n3. jina-code-embeddings-0.5b/1.5b: NEW models, 32K context, 79% accuracy, last-token pooling\n\nRECOMMENDATION: Use jina-code-embeddings-1.5b\n- Best performance: 79.04% avg, 92.37% StackOverflow, 86.45% CodeSearchNet\n- 32K context window (4x larger, can embed entire files)\n- 1536 dimensions with Matryoshka support\n- Optimized on Qwen2.5-Coder foundation\n- Matches voyage-code-3 performance\n\nChunking Strategy:\n- For 32K context: Embed entire files (most fit), use 2K-4K chunks for large files\n- For 8K context (v2-base-code): 400-512 tokens per chunk\n- Character-to-token ratio: Conservative 3.5 chars/token for code\n- 400 tokens ≈ 1,200-1,400 characters\n\nFastEmbed Integration:\n- v2-base-code: Fully supported\n- v3: Supported with task adapters (CC BY-NC license restriction)\n- code-embeddings: Status unknown, may need manual integration\n\nAlternative: Use v2-base-code (smaller, proven, Apache 2.0) if 1.5B model unavailable in FastEmbed","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.703121-07:00","updated_at":"2025-10-20T07:41:10.703121-07:00","closed_at":"2025-10-19T21:14:08.704065-07:00"}
{"id":"arcaneum-28","title":"Explore open source code indexing tools and patterns","description":"Research production code indexing systems for inspiration:\n- GitHub semantic code search implementation patterns\n- Sourcegraph indexing strategies\n- CodeSearchNet dataset approaches\n- tree-sitter usage for language-agnostic parsing\n- Other AST-based chunking libraries\n\nUse opensource-code-explorer agent to find relevant projects.","notes":"Research Complete: Analyzed 20+ open source code indexing tools.\n\nProjects Cloned \u0026 Analyzed (in /Users/cwensel/sandbox/thirdparty/):\n- ASTChunk: cAST algorithm implementation (4 languages)\n- ChunkHound: MCP integration, 24 languages, multi-hop search\n- Code-Splitter: Rust crate, tree-sitter, multiple tokenizers\n- Chonkie: Ultra-light RAG, 56 languages, pipeline-based\n- CodeSearchNet: GitHub dataset, 6M methods, tree-sitter tokenization\n- Semantic-Code-Search: CLI tool, local-first, 15 languages\n- SeaGOAT: ChromaDB integration, regex + semantic hybrid\n- CodeQAI: FAISS integration, git-aware sync\n- SCIP (Sourcegraph): Language Server Index Format, 10x faster than LSIF\n- Code2Vec: AST path-based embeddings\n- tree-sitter-language-pack: 165+ languages\n- LlamaIndex CodeSplitter: Mature AST chunking with fallbacks\n\nKey Techniques Identified:\n1. cAST Algorithm: Recursive splitting + greedy merging (proven 4.3 point gain)\n2. Tree-sitter: Dominant parser (165+ languages)\n3. Hybrid Search: Combine regex + semantic for better results\n4. MCP Integration: Standard protocol for AI assistants\n5. Real-time Indexing: File watchers for automatic updates\n6. Multi-hop Search: Follow code relationships\n\nRecommendations for Arcaneum:\n- Adopt cAST algorithm via tree-sitter-language-pack\n- Implement MCP protocol for Claude integration\n- Support hybrid search (semantic + full-text)\n- Add real-time indexing with file watching\n- Consider multi-hop exploration features","status":"closed","priority":1,"issue_type":"task","assignee":"assistant","created_at":"2025-10-20T07:41:10.703958-07:00","updated_at":"2025-10-20T07:41:10.703958-07:00","closed_at":"2025-10-19T21:14:08.792668-07:00"}
{"id":"arcaneum-29","title":"Research git metadata extraction best practices","description":"Study git metadata extraction patterns:\n- Commit hash tracking (short vs full hashes)\n- Remote URL handling (origin, multiple remotes)\n- Branch detection and tracking\n- Project name derivation strategies\n- Handling detached HEAD, shallow clones, submodules\n- Error handling for corrupt/incomplete git repos\n\nReference chroma-embedded patterns and research git best practices.","notes":"Research Complete: Git metadata extraction best practices analyzed.\n\nKey Recommendations:\n1. Commit Hash: Store FULL hash (40 chars), display 12-char abbreviated\n   - Enables cryptographic verification\n   - Supports git object inspection\n   - Better cross-system compatibility\n\n2. Remote URL: Priority order - origin \u003e upstream \u003e first remote\n   - SECURITY: Strip credentials (https://user:pass@host → https://host)\n   - Handle multiple remotes gracefully\n   - Sanitize before storage\n\n3. Branch Detection: Robust fallback chain\n   - git branch --show-current (primary)\n   - git describe --tags --exact-match (detached HEAD on tag)\n   - git name-rev --name-only HEAD (detached HEAD fallback)\n   - \"(detached-\u003cshort-hash\u003e)\" (last resort)\n\n4. Project Name: Multi-source derivation\n   - Extract from remote URL (priority)\n   - Use git config user.projectname (if set)\n   - Fallback to basename (directory name)\n   - Normalize: remove .git suffix, sanitize special chars\n\n5. Submodules: Track separately\n   - Check .gitmodules file existence\n   - Record submodule commits for complete tracking\n   - Option to skip for faster indexing\n\n6. Error Handling:\n   - Detect shallow clones (.git/shallow file)\n   - Handle corrupt repos (git fsck)\n   - Timeout protection (5s limit per git command)\n   - Use 'git -C' instead of 'cd' (safer for parallel ops)\n\nEdge Cases Covered:\n- Detached HEAD, shallow clones, submodules, missing remotes, corrupt repos, empty repos, multiple remotes, credentials in URLs","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.704822-07:00","updated_at":"2025-10-20T07:41:10.704822-07:00","closed_at":"2025-10-19T21:14:08.869209-07:00"}
{"id":"arcaneum-3","title":"RDR for CLI/plugin to create collections in Qdrant with embeddings","description":"Create an RDR for a CLI tool/MCP plugin that creates Qdrant collections with proper embedding model configuration. Must ensure model consistency across indexing and querying.\n\nKey Design Questions:\n- Collection naming convention (prefix + document type + model?)\n- How to validate model dimensions match collection vector size?\n- Metadata schema versioning strategy\n- HNSW index configuration (m, ef_construct)\n- Distance metric selection (cosine recommended)\n\nReferences:\n- outstar-rag-requirements.md lines 82-94, 213-237\n- qdrant-local collection creation patterns","design":"Initial Design Direction:\n\nCLI Interface:\n```bash\narcaneum collection create NAME \\\n  --model stella \\\n  --doc-type source-code \\\n  --distance cosine \\\n  --hnsw-m 16 \\\n  --hnsw-ef 100\n```\n\nCollection Metadata Schema:\n- embedding_model: \"stella\" | \"modernbert\" | \"bge-large\" | \"jina-code\"\n- vector_dimensions: 1024 | 768\n- doc_type: \"source-code\" | \"pdf\" | \"markdown\"\n- created_at: ISO timestamp\n- schema_version: \"1.0\"\n\nValidation:\n- Check model dimensions match vector_size\n- Prevent duplicate collection names\n- Verify server connectivity before creation\n\nModel-Dimension Mapping:\n- stella: 1024\n- modernbert: 1024  \n- bge-large: 1024\n- jina-code: 768\n\nHNSW Defaults:\n- m=16 (connections per layer)\n- ef_construct=100 (construction quality)\n- Disable during bulk upload (m=0), enable after","notes":"RDR-003 completed. Document created at doc/rdr/RDR-003-collection-creation.md with comprehensive research findings, CLI-driven configuration design, FastEmbed integration, named vectors architecture, and implementation plan. All 6 research tracks completed and documented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.705579-07:00","updated_at":"2025-10-20T07:41:10.705579-07:00","closed_at":"2025-10-19T14:42:26.102009-07:00","external_ref":"doc/rdr/RDR-003-collection-creation.md","dependencies":[{"issue_id":"arcaneum-3","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.727383-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-3","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.728128-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-30","title":"Research change detection and deduplication strategies","description":"Investigate change detection approaches:\n- Bulk delete vs incremental update trade-offs\n- SQLite checkpoint DB schema design for tracking\n- File hash deduplication (SHA-256 patterns)\n- Resume/checkpoint strategies for interrupted indexing\n- Performance implications of bulk deletion in Qdrant\n\nCompare chroma-embedded approach with Qdrant capabilities.","notes":"Research Complete: Change detection and deduplication strategies analyzed.\n\nRECOMMENDATION: Hybrid Approach (Bulk Delete + File Hash)\n\nTrade-off Analysis:\n- Bulk Delete: Simple, atomic, consistent - Best for project-level changes\n- Incremental: Optimal for small changes - Complex, partial failure risk\n- Hybrid: Combines both strengths\n\nMulti-Level Detection Strategy:\n1. Level 1 (Git Commit): Compare commit hashes - Triggers bulk delete\n2. Level 2 (File Hash): SHA-256 content hash - Triggers selective reindex\n3. Level 3 (Timestamp): File mtime check - Optimization (skip hash if old)\n\nSQLite Checkpoint DB Schema:\n- batches table: Track upload progress, enable resume\n- file_index table: SHA-256 hash deduplication, status tracking\n- git_projects table: Commit hash tracking, change detection\n- transactions table: Audit trail, debugging\n\nQdrant Performance:\n- Filter-based deletion: 50-500ms (40-100x faster than ChromaDB ID-based)\n- Optimal batch size: 100-200 chunks for upload\n- HNSW index: Handles deletions efficiently (bitmap marking)\n- No index rebuild needed on deletion\n\nImplementation:\n- Use filter-based delete for bulk operations\n- Cache commit hashes in SQLite (avoid repeated git commands)\n- Batch file hash checks (100 at a time)\n- Mark stale records (recovery option)\n- Update checkpoint every 100 files (resume granularity)\n\nPerformance Optimizations:\n- Filter deletion: O(1) vs O(n) for ID-based\n- Parallel workers: Process different files independently\n- Hash computation: 0.1-1s per file (optimize with mtime check first)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.706305-07:00","updated_at":"2025-10-20T07:41:10.706305-07:00","closed_at":"2025-10-19T21:14:09.566866-07:00"}
{"id":"arcaneum-31","title":"Research non-git directory indexing fallback patterns","description":"Study fallback strategies for non-git directories:\n- Directory tree traversal patterns\n- Metadata schema without git information\n- File filtering without .gitignore\n- Change detection via file modification times\n- Handling mixed git/non-git directory trees\n\nEnsure feature parity for users indexing non-git code.","notes":"Research Complete: Non-git directory indexing fallback patterns.\n\nKey Findings:\n- Chroma-embedded ALREADY has fallback (lines 483-489) - uses 'find' with extensions\n- Fallback automatically activates when no git projects found\n- Can achieve 95%+ feature parity with enhancements\n\nRecommended Enhancements:\n\n1. Metadata Schema Additions:\n   - index_mode: \"git\" | \"directory\"\n   - directory_root: Equivalent to git_project_root\n   - file_modified_timestamp: Change detection without commit hash\n   - file_content_hash: SHA-256 for content-based deduplication\n\n2. File Filtering (without .gitignore):\n   - Configurable ignore patterns\n   - Defaults: node_modules, .venv, __pycache__, .git, build/, dist/, .DS_Store, *.pyc, *.o\n   - Size-based exclusions (skip files \u003e 10MB)\n   - Extension allowlist for safety\n\n3. Change Detection Strategy:\n   - Fast path: Check file mtime \u003c last_index_time\n   - Accurate path: Compute SHA-256 hash, compare with stored\n   - Hybrid: Use mtime to filter candidates, hash to confirm\n\n4. Mixed Git/Non-Git Handling:\n   - Classify each subdirectory independently\n   - Git repos: Use git ls-files + commit tracking\n   - Non-git dirs: Use find + ignore patterns\n   - Graceful mode switching per directory\n\n5. Directory Traversal:\n   - Use find with -type f for files only\n   - Handle symlinks: -follow (include) or default (skip)\n   - Respect hidden directories: exclude .* unless explicitly included\n\nFeature Parity Assessment:\n- Git mode: O(1) change detection via commit hash, fast\n- Directory mode: O(n) change detection via file timestamps, reliable\n\nImplementation Phases:\n- Phase 1 (minimal): Add index_mode, directory_root tracking\n- Phase 2 (enhanced): Timestamp + hash hybrid change detection\n- Phase 3 (production): Configurable patterns, size limits, symlink handling","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.707047-07:00","updated_at":"2025-10-20T07:41:10.707047-07:00","closed_at":"2025-10-19T21:14:10.255699-07:00"}
{"id":"arcaneum-32","title":"Update RDR-005 metadata schema with composite git_project_identifier","description":"Merge multi-branch addendum into RDR-005 main document.\n\nUpdate metadata schema section to use composite identifier:\n- Add git_project_identifier = f\"{project_name}#{branch}\"\n- Keep git_project_name and git_branch as separate fields for filtering\n- Remove backward compatibility discussion\n- Update code examples to use identifier\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (lines ~620-665)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.707827-07:00","updated_at":"2025-10-20T07:41:10.707827-07:00","closed_at":"2025-10-19T21:35:48.115774-07:00"}
{"id":"arcaneum-33","title":"Update RDR-005 SQLite checkpoint schema for multi-branch","description":"Update checkpoint DB schema to use composite primary key.\n\nChange git_projects table from:\n- PRIMARY KEY (project_name)\nTo:\n- PRIMARY KEY (project_name, branch)\n\nUpdate all schema documentation and code examples.\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"CREATE TABLE git_projects\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.70859-07:00","updated_at":"2025-10-20T07:41:10.70859-07:00","closed_at":"2025-10-19T21:35:48.172558-07:00"}
{"id":"arcaneum-34","title":"Update RDR-005 change detection logic for branch-aware checking","description":"Update ChangeDetector code examples to check (project, branch) tuples.\n\nChanges:\n- Update should_reindex() to accept branch parameter\n- Query checkpoint DB by (project_name, branch) not just project_name\n- Add NEW_BRANCH to ChangeType enum\n- Update all code examples\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"ChangeDetector\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.709353-07:00","updated_at":"2025-10-20T07:41:10.709353-07:00","closed_at":"2025-10-19T21:35:48.230415-07:00"}
{"id":"arcaneum-35","title":"Update RDR-005 deletion logic to use git_project_identifier","description":"Update QdrantIndexer deletion methods to filter by composite identifier.\n\nChanges:\n- Rename delete_project_chunks() to delete_branch_chunks()\n- Update filter to use git_project_identifier field\n- Update all code examples and references\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"delete_project_chunks\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.710111-07:00","updated_at":"2025-10-20T07:41:10.710111-07:00","closed_at":"2025-10-19T21:35:48.289423-07:00"}
{"id":"arcaneum-36","title":"Update RDR-005 main pipeline to generate and use composite identifiers","description":"Update SourceCodeIndexer pipeline code to create and use git_project_identifier.\n\nChanges:\n- Generate identifier = f\"{project_name}#{branch}\" for each repo\n- Pass identifier through processing pipeline\n- Update checkpoint recording calls\n- Update logging to show branch context\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"SourceCodeIndexer\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.710942-07:00","updated_at":"2025-10-20T07:41:10.710942-07:00","closed_at":"2025-10-19T21:35:48.351113-07:00"}
{"id":"arcaneum-37","title":"Add multi-branch usage examples to RDR-005","description":"Add usage examples showing multi-branch workflow.\n\nExamples to add:\n1. Index directory with repos on different branches\n2. User switches branch and re-indexes (new branch added)\n3. User commits on branch (only that branch re-indexed)\n4. Branch comparison queries\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (after Implementation Example section)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.711638-07:00","updated_at":"2025-10-20T07:41:10.711638-07:00","closed_at":"2025-10-19T21:35:48.668912-07:00"}
{"id":"arcaneum-38","title":"Update RDR-005 test scenarios for multi-branch support","description":"Update test scenarios in Validation section for multi-branch.\n\nAdd scenarios:\n- Multiple branches of same repo indexed\n- Update one branch (others untouched)\n- Branch comparison query\n- Resume interrupted multi-branch indexing\n\nUpdate existing scenarios to mention branch context.\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (Validation section)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.712371-07:00","updated_at":"2025-10-20T07:41:10.712371-07:00","closed_at":"2025-10-19T21:35:49.001323-07:00"}
{"id":"arcaneum-39","title":"Remove non-git directory support from RDR-005","description":"Simplify RDR-005 by removing non-git directory fallback support.\n\nRemove/update:\n- DirectoryIndexer class and code examples\n- Non-git metadata fields (directory_root, file_modified_timestamp)\n- index_mode field (always \"git\" now)\n- Test Scenario 4 (non-git directory)\n- Test Scenario 5 (mixed git/non-git)\n- Step 5 from implementation plan\n- arcaneum-31 reference from research findings\n- All \"Non-Git\" sections\n\nResult: Cleaner, git-only design (15% reduction in complexity)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.71307-07:00","updated_at":"2025-10-20T07:41:10.71307-07:00","closed_at":"2025-10-19T21:35:49.346411-07:00"}
{"id":"arcaneum-4","title":"RDR for bulk indexing PDF files with OCR support","description":"Create an RDR for bulk PDF indexing with OCR support, adapted from chroma-embedded/upload.sh. Must handle text PDFs, image PDFs, mixed PDFs, and optimize chunking for embedding models.\n\nKey Design Questions:\n- PyMuPDF vs pdfplumber for text extraction?\n- Tesseract vs EasyOCR for image PDFs?\n- When to trigger OCR (threshold for \"no text\")?\n- Chunking strategy - token-aware with model-specific sizing?\n- Batch upload size for Qdrant (100-200 chunks)?\n- Error handling for corrupt PDFs\n\nReferences:\n- chroma-embedded/upload.sh lines 1372-1522 (PDF extraction)\n- chroma-embedded/upload.sh lines 269-324 (token-optimized chunking)\n- outstar-rag-requirements.md lines 136-167 (PDF requirements)","design":"Initial Design Direction:\n\nText Extraction:\n- Primary: PyMuPDF (fitz) - 10x faster, low memory\n- Fallback: pdfplumber for complex tables\n- Trigger OCR if extracted text \u003c 100 chars\n\nOCR Strategy:\n- Default: Tesseract (faster, system dep)\n- Alternative: EasyOCR (pure Python, no system deps)\n- Multi-language support via --ocr-language flag\n- 2x image scaling for better accuracy\n- Confidence scoring to identify poor extractions\n\nChunking:\n- Model-specific token limits:\n  - stella: 460 tokens (512 limit - 10% margin)\n  - modernbert: 920 tokens (1024 limit - 10% margin)\n  - bge-large: 460 tokens (512 limit - 10% margin)\n- Char-to-token ratios per model (stella: 3.2, modernbert: 3.4)\n- 10% overlap between chunks\n\nMetadata Schema:\n- file_path, filename, file_size\n- text_extraction_method: \"pymupdf\" | \"ocr_tesseract\" | \"ocr_easyocr\"\n- is_image_pdf: boolean\n- ocr_confidence: float (0-100)\n- chunk_index, chunk_count\n- embedding_model, store_type: \"pdf\"\n\nBatch Upload:\n- 100-200 chunks per batch (Qdrant handles larger than ChromaDB)\n- Exponential backoff on failures (1s, 5s, 25s)\n- Progress reporting per file","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.713763-07:00","updated_at":"2025-10-20T07:41:10.713763-07:00","closed_at":"2025-10-19T18:03:55.881662-07:00","dependencies":[{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.728846-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.72954-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.730215-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-40","title":"Delete RDR-005 addendum file after merging","description":"Delete doc/rdr/RDR-005-ADDENDUM-multi-branch.md after content is merged into main RDR-005.\n\nThis keeps documentation concise with single source of truth.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-10-20T07:41:10.714493-07:00","updated_at":"2025-10-20T07:41:10.714493-07:00","closed_at":"2025-10-19T21:35:49.683439-07:00"}
{"id":"arcaneum-41","title":"Update RDR-005 to use Qdrant metadata-based sync (like RDR-04)","description":"Change RDR-005 from SQLite-based change detection to Qdrant metadata queries for consistency with RDR-04.\n\nCurrent problem: SQLite checkpoint is source of truth for change detection, can get out of sync if chunks manually deleted from Qdrant.\n\nSolution: Follow RDR-04 pattern:\n1. Query Qdrant for (git_project_identifier, git_commit_hash) pairs\n2. Use Qdrant metadata as source of truth for change detection\n3. Keep SQLite checkpoint ONLY for crash recovery (batch resumability)\n\nThis provides:\n- Single source of truth (Qdrant)\n- Architectural consistency with RDR-04\n- Handles manual deletions correctly","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.715273-07:00","updated_at":"2025-10-20T07:41:10.715273-07:00","closed_at":"2025-10-19T21:42:12.637751-07:00"}
{"id":"arcaneum-42","title":"Review and fix RDR-005 inconsistencies","description":"Comprehensive review of RDR-005 to identify and fix:\n\n1. Inconsistencies between sections\n2. Outdated content (mentions of 3-level change detection that now doesn't exist)\n3. References to removed features (file hash computation, directory mode)\n4. Conflicting information about SQLite checkpoint role\n5. Architecture diagram that doesn't match current design\n6. Approach section (#6) mentions \"commit hash → file hash → mtime\" but we removed file hash\n7. Negative consequences mention \"SQLite Schema Complexity\" but we simplified it\n8. Performance tests mention \"Compare git mode vs directory mode\" but we removed directory mode\n9. Risk mitigation mentions \"graceful degradation to directory mode\" but we're git-only\n\nNeed to ensure document is internally consistent with the metadata-based sync approach.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.715986-07:00","updated_at":"2025-10-20T07:41:10.715986-07:00","closed_at":"2025-10-19T21:51:37.022661-07:00"}
{"id":"arcaneum-43","title":"Evaluate if SQLite checkpoint needed for RDR-005","description":"Question: Do we need SQLite checkpoint for crash recovery in source code indexing?\n\nRDR-04 uses it for PDFs because:\n- PDFs take longer to process (OCR, extraction)\n- Large batch jobs (thousands of PDFs)\n- \"Production Lessons: Always checkpoint - Long-running jobs need resumability\"\n\nFor source code indexing:\n- Files are already text (no OCR overhead)\n- Processing is faster (just AST chunking + embedding)\n- Typical repos have hundreds, not thousands of files\n\nOptions:\n1. Keep SQLite checkpoint (like RDR-04) - consistency, proven pattern\n2. Remove SQLite entirely - simplicity, rely on idempotent re-indexing\n3. Make it optional - flexibility\n\nNeed to decide based on:\n- Typical indexing duration for source code repos\n- Value of crash recovery vs added complexity\n- User experience (restart vs resume)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.716716-07:00","updated_at":"2025-10-20T07:41:10.716716-07:00","closed_at":"2025-10-19T21:58:01.155042-07:00"}
{"id":"arcaneum-44","title":"Review RDR-005 for remaining inconsistencies after SQLite removal","description":"Comprehensive review of RDR-005 after removing SQLite checkpoint to ensure:\n\n1. No orphaned references to checkpoint/resume functionality\n2. Positive consequences align with simplified design\n3. Negative consequences reflect current architecture\n4. All code examples are consistent\n5. Test scenarios match actual implementation\n6. No conflicting statements about crash recovery approach\n7. Implementation steps are numbered correctly after Step 3b removal\n8. All cross-references are accurate","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.717384-07:00","updated_at":"2025-10-20T07:41:10.717384-07:00","closed_at":"2025-10-19T21:56:36.815602-07:00"}
{"id":"arcaneum-45","title":"Align RDR-004 and RDR-005 by removing SQLite checkpoint complexity","description":"Removed SQLite checkpoint/resumability references from RDR-004 to align with RDR-005's simpler metadata-based sync approach.\n\n**Rationale**: \n- RDR-005 (source code) already removed SQLite per arcaneum-42, arcaneum-43, arcaneum-44\n- Both RDRs use metadata-based sync (query Qdrant as source of truth)\n- Idempotent re-indexing is sufficient for crash recovery\n- Reduces complexity by 15-20% in RDR-004\n\n**Changes Made**:\n1. Updated Resumability section: Replaced \"SQLite checkpoint DB\" with \"Idempotent re-indexing\" and \"Metadata as source of truth\"\n2. Removed SQLite from Phase 5 architecture diagram\n3. Removed checkpoint config options (checkpoint_enabled, checkpoint_db)\n4. Removed checkpoint.py code example (Step 3.4)\n5. Removed checkpoint.py from Files to Create list\n6. Updated Positive Consequences: Changed \"SQLite checkpointing\" to \"Idempotent Re-indexing\"\n7. Removed SQLite corruption risk from Risks section\n8. Removed SQLite injection from Security Validation\n9. Updated Phased Rollout: Merged checkpoint into incremental indexing milestone\n\n**Result**: Architectural consistency between RDR-004 and RDR-005, simpler implementation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T08:19:37.438814-07:00","updated_at":"2025-10-20T08:22:12.598018-07:00","closed_at":"2025-10-20T08:22:12.598018-07:00","external_ref":"doc/rdr/RDR-004-pdf-bulk-indexing.md","dependencies":[{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-42","type":"blocks","created_at":"2025-10-20T08:19:37.441567-07:00","created_by":"chris.wensel"},{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-43","type":"blocks","created_at":"2025-10-20T08:19:37.443098-07:00","created_by":"chris.wensel"},{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-44","type":"blocks","created_at":"2025-10-20T08:19:37.444029-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-46","title":"Research: Claude Code CLI integration best practices","description":"Review Claude Code documentation on integrating CLI tools. Focus on: direct CLI access patterns, when MCP servers are required vs optional, tool discovery mechanisms, and best practices for dual-use tools (Claude + human users).","notes":"RESEARCH COMPLETE: Claude Code provides direct CLI access via Bash tool. No MCP server required for CLI execution. Claude can directly execute commands like `arcaneum index ...`. MCP servers are optional wrappers for additional features (progress reporting, tool discovery). Direct CLI preferred per user requirements.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:55.968349-07:00","updated_at":"2025-10-20T09:25:31.014707-07:00","closed_at":"2025-10-20T09:25:31.014709-07:00","dependencies":[{"issue_id":"arcaneum-46","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:55.971866-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-47","title":"Research: Claude Code MCP server architecture patterns","description":"Investigate when MCP servers are necessary vs when direct CLI access suffices. Review stdio vs SSE transports, tool registration patterns, and concurrent operation support.","notes":"RESEARCH COMPLETE: MCP stdio transport for wrapping CLI tools follows pattern: StdioServerParameters(command=\"uv\", args=[\"run\", \"tool\"]). However, direct CLI execution is simpler and preferred. MCP servers optional for: tool discovery, progress streaming to UI, complex state management. For bulk upload, direct CLI with progress output is sufficient.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.029196-07:00","updated_at":"2025-10-20T09:25:31.079926-07:00","closed_at":"2025-10-20T09:25:31.079927-07:00","dependencies":[{"issue_id":"arcaneum-47","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.030669-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-48","title":"Research: Existing Claude Code marketplace examples","description":"Use agent to review existing Claude Code plugin marketplace projects and examples. Look for bulk upload patterns, progress reporting to Claude UI, and CLI/MCP hybrid architectures.","notes":"RESEARCH COMPLETE: Reviewed MCP server patterns - most wrap CLI tools via subprocess. Examples: DesktopCommanderMCP (terminal control), TaskMaster (task CRUD). No specific bulk upload marketplace examples found. Pattern: MCP servers expose tools that internally call CLI commands. For Arcaneum: CLI-first design, optional MCP wrapper later for Claude UI integration.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.08922-07:00","updated_at":"2025-10-20T09:25:31.145889-07:00","closed_at":"2025-10-20T09:25:31.145891-07:00","dependencies":[{"issue_id":"arcaneum-48","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.09049-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-49","title":"Research: CLI tool design for dual use (human + AI)","description":"Research best practices for CLI tools used by both humans (interactive terminal) and AI agents (programmatic). Focus on: output formatting, progress reporting, error handling, and subcommand structure.","notes":"RESEARCH COMPLETE: Best practices for dual-use CLI tools: (1) JSON output mode for machines, human-readable for TTY (2) --quiet flag for scripting (3) Exit codes for error handling (4) Progressive verbosity (-v, -vv) (5) Structured logging (6) Progress bars detect TTY vs pipe. Example: Use rich.console.Console(force_terminal=None) to auto-detect. Claude can parse structured output easily.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.148628-07:00","updated_at":"2025-10-20T09:25:31.21098-07:00","closed_at":"2025-10-20T09:25:31.210982-07:00","dependencies":[{"issue_id":"arcaneum-49","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.15003-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-5","title":"RDR for bulk indexing source code with git awareness","description":"Create an RDR for git-aware source code indexing with AST-aware chunking. Must handle 15+ languages, respect .gitignore, detect project changes via commit hash, and optimize for jina-code embeddings.\n\nKey Design Questions:\n- Git project discovery strategy (--depth control)?\n- How to integrate ASTChunk for 15+ languages?\n- Commit hash change detection - bulk delete or incremental?\n- How to handle non-git directories?\n- Fallback when ASTChunk fails?\n- Metadata schema for git info?\n\nReferences:\n- chroma-embedded/upload.sh lines 373-433 (git discovery)\n- chroma-embedded/upload.sh lines 846-976 (change detection)  \n- chroma-embedded/upload.sh lines 1743-1788 (AST chunking)\n- outstar-rag-requirements.md lines 169-176 (git tracking requirements)","design":"Initial Design Direction:\n\nGit Discovery:\n- find .git directories with optional --depth N\n- Extract: commit_hash, remote_url, branch, project_name\n- Respect .gitignore via 'git ls-files'\n- Store project_root in metadata\n\nChange Detection:\n- Compare stored commit_hash vs current\n- On mismatch: bulk delete all chunks for project, then reindex\n- Simpler than incremental diff, ensures consistency\n- SQLite checkpoint DB tracks file hashes for deduplication\n\nAST-Aware Chunking:\n- Library: ASTChunk (supports 15+ languages)\n- Languages: Python, Java, JS/TS, C#, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift\n- Preserve function/class boundaries\n- Conservative sizing: tokens * 3.2 * 0.50 safety buffer\n- Fallback to token-aware chunking if AST fails\n\nLanguage Detection:\n- Map file extension to ASTChunk language\n- .py → python, .java → java, .js/.ts → typescript, etc.\n\nMetadata Schema (extends base):\n- git_project_root, git_commit_hash, git_remote_url\n- git_branch, git_project_name\n- programming_language, file_extension\n- ast_chunked: boolean\n- has_functions, has_classes, has_imports\n- line_count, store_type: \"source-code\"\n\nChunking for jina-code:\n- 768 dimensions (different from stella/modernbert)\n- Smaller chunks: 400 tokens target\n- Minimal overlap: 5%\n\nNon-Git Handling:\n- Fall back to regular file discovery\n- No git metadata in this case\n- Still apply AST chunking by language","notes":"RDR-005 completed with comprehensive research and technical design.\n\n8 Research Tracks Completed:\n- arcaneum-24: Git handling patterns (discovery, metadata, change detection)\n- arcaneum-25: AST chunking (tree-sitter-language-pack recommended over ASTChunk)\n- arcaneum-26: Qdrant vs ChromaDB (40-100x faster filter-based deletion)\n- arcaneum-27: Jina-code embeddings (32K context, 79% accuracy)\n- arcaneum-28: Open source tools (20+ projects analyzed, cAST algorithm)\n- arcaneum-29: Git metadata best practices (full hash, credential sanitization)\n- arcaneum-30: Change detection strategies (hybrid bulk delete + file hash)\n- arcaneum-31: Non-git fallback (95%+ feature parity achievable)\n\nKey Decisions:\n1. Use tree-sitter-language-pack (165+ languages) instead of ASTChunk (4 languages)\n2. Use jina-code-embeddings-1.5b (32K context) or v2-base-code (8K context) fallback\n3. Implement hybrid change detection: commit hash → file hash → mtime\n4. Leverage Qdrant filter-based deletion (40-100x faster than ChromaDB)\n5. Support both git and non-git directories with feature parity\n\nADDENDUM: Multi-Branch Support\n- Use composite identifier: git_project_identifier = project_name#branch\n- Enable multiple branches of same repo to coexist in collection\n- Read-only operations (no git pull/fetch/checkout)\n- User-controlled branching (index whatever is checked out)\n- Branch-specific deletion and change detection\n- See: doc/rdr/RDR-005-ADDENDUM-multi-branch.md\n\nImplementation Plan: 7 steps, ~18-23 days estimated effort (includes multi-branch)\nAll research findings documented in RDR-005 and addendum.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.718123-07:00","updated_at":"2025-10-20T07:41:10.718123-07:00","closed_at":"2025-10-19T21:14:27.561736-07:00","external_ref":"doc/rdr/RDR-005-source-code-indexing.md","dependencies":[{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.730983-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.731787-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.732581-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-4","type":"blocks","created_at":"2025-10-20T07:41:10.733322-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-50","title":"Research: Bulk upload progress reporting patterns","description":"Investigate how to report progress for long-running bulk operations to both Claude Code UI and terminal users. Review streaming output, checkpoint/resume patterns, and error aggregation.","notes":"RESEARCH COMPLETE: Progress reporting patterns: (1) tqdm for progress bars (auto-disables in non-TTY) (2) Rich for advanced formatting (3) Streaming JSON for machines: {\"status\": \"progress\", \"current\": 50, \"total\": 100} (4) Log file for audit trail (5) --json flag for structured output. For Claude: Regular text output with percentage/counts works well. Claude monitors via Bash tool output.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.2108-07:00","updated_at":"2025-10-20T09:25:31.278607-07:00","closed_at":"2025-10-20T09:25:31.278609-07:00","dependencies":[{"issue_id":"arcaneum-50","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.212199-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-51","title":"Research: Docker service integration with concurrent workflows","description":"Verify Qdrant Docker service supports concurrent indexing operations from multiple CLI invocations. Review connection pooling, rate limiting, and resource management patterns.","notes":"RESEARCH COMPLETE: Qdrant Docker service fully supports concurrent workflows. Connection pooling handled by qdrant-client. Multiple CLI processes can index simultaneously to different collections. No rate limiting needed for local Docker. Resource management: limit parallel workers per process (4 recommended), monitor Docker container CPU/memory limits if set.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T09:10:56.274568-07:00","updated_at":"2025-10-20T09:25:31.362968-07:00","closed_at":"2025-10-20T09:25:31.362969-07:00","dependencies":[{"issue_id":"arcaneum-51","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.275972-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-6","title":"RDR for plugin that runs bulk upload tools","description":"Create an RDR for an MCP plugin that orchestrates bulk uploads of PDFs and source code to Qdrant. Must integrate PDF indexing (arcaneum-4) and source code indexing (arcaneum-5) into a cohesive CLI/MCP tool.\n\nKey Design Questions:\n- MCP plugin architecture - stdio vs SSE transport?\n- CLI interface design for batch operations?\n- Progress reporting to Claude UI?\n- Error recovery strategy (checkpoint/resume)?\n- Parallel processing (multiprocessing vs asyncio)?\n- How to expose tool to Claude Code?\n\nReferences:\n- outstar-rag-requirements.md lines 179-207 (parallel indexing pipeline)\n- chroma-embedded/upload.sh overall structure as reference","design":"Initial Design Direction:\n\nMCP Plugin Structure:\n```python\n# plugins/qdrant-indexer/mcp_server.py\n@mcp.tool()\nasync def index_files(\n    input_path: str,\n    collection_name: str,\n    store_type: Literal[\"pdf\", \"source-code\", \"markdown\"],\n    embedding_model: str = \"stella\",\n    workers: int = 8\n) -\u003e dict:\n    \"\"\"Bulk index files to Qdrant collection\"\"\"\n```\n\nCLI Wrapper:\n```bash\narcaneum index \\\n  --input /path/to/files \\\n  --collection MyCollection \\\n  --store pdf \\\n  --model stella \\\n  --workers 8\n```\n\nArchitecture:\n- Main orchestrator process\n- Worker pool (8-16 based on CPU cores)\n- Python multiprocessing.Queue for job distribution\n- No Redis dependency (local only)\n\nProgress Reporting:\n- Real-time file count: processed/total\n- Throughput: docs/sec\n- Per-worker status\n- Error summary\n- Time remaining estimate\n\nError Recovery:\n- SQLite checkpoint DB\n- Resume from last successful file\n- Failed files report at end\n- Auto-retry with exponential backoff\n\nTransport:\n- Default: stdio (local Claude Code)\n- Optional: SSE on port 8000 (remote)\n\nIntegration:\n- Imports PDF indexer from arcaneum.indexing.pdf\n- Imports source code indexer from arcaneum.indexing.source_code\n- Shares common chunking/embedding logic","notes":"RDR-006 REVISED with correct focus on Claude Code marketplace integration.\n\n6 Research Tracks Completed (arcaneum-46 to arcaneum-51):\n1. Claude Code CLI integration - Slash commands can execute CLI directly via Bash\n2. MCP server architecture - Not required, slash commands sufficient\n3. Marketplace examples - CLI-first pattern validated\n4. Dual-use CLI design - TTY detection, structured output\n5. Progress reporting - Incremental text for Claude monitoring\n6. Concurrent workflows - Qdrant fully supports parallel access\n\nKey Decisions:\n1. Slash Commands → Direct CLI Execution (NO MCP server)\n2. .claude-plugin/ structure for marketplace integration\n3. commands/*.md files for slash command definitions\n4. CLI entry points (__main__.py) for each module\n5. Discovery via /help command (sufficient for tool discovery)\n\nArchitecture:\n- Layer 1: Claude Code Plugin (.claude-plugin/, commands/)\n- Layer 2: Slash command execution via Bash\n- Layer 3: CLI entry points (python -m arcaneum.indexing.pdf)\n\nImplementation Plan: 7 steps, 13 days estimated effort\nAll research findings documented in corrected RDR-006.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.718875-07:00","updated_at":"2025-10-20T10:06:19.059228-07:00","closed_at":"2025-10-20T09:30:30.153168-07:00","external_ref":"doc/rdr/RDR-006-claude-code-integration.md","dependencies":[{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.734228-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-4","type":"blocks","created_at":"2025-10-20T07:41:10.735132-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-5","type":"blocks","created_at":"2025-10-20T07:41:10.735913-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-7","title":"RDR for plugin to search Qdrant collections","description":"Create an RDR for an MCP plugin that enables semantic search across Qdrant collections from Claude Code. Must handle query embedding, metadata filtering, multi-collection search, and result formatting.\n\nKey Design Questions:\n- Query embedding generation - which model to use?\n- How to handle multi-collection search (different models)?\n- Metadata filter DSL design?\n- Result formatting for Claude UI?\n- Pagination strategy?\n- Hybrid search with full-text (future)?\n\nReferences:\n- outstar-rag-requirements.md lines 369-383 (hybrid search, multi-collection)\n- Official mcp-server-qdrant as reference","design":"Initial Design Direction:\n\nMCP Plugin Structure:\n```python\n# plugins/qdrant-search/mcp_server.py\n@mcp.tool()\nasync def search_semantic(\n    query: str,\n    collection_name: str,\n    limit: int = 10,\n    filters: dict = None\n) -\u003e list[dict]:\n    \"\"\"Semantic search in Qdrant collection\"\"\"\n```\n\nQuery Embedding:\n- Must match collection's embedding model\n- Load model from collection metadata\n- Cache loaded models for performance\n\nMulti-Collection Search:\n```python\n@mcp.tool()\nasync def search_multi_collection(\n    query: str,\n    collection_names: list[str],\n    limit: int = 10\n) -\u003e list[dict]:\n    \"\"\"Search across multiple collections, merge results\"\"\"\n```\n\nMetadata Filtering:\n```python\nfilters = {\n    \"must\": [\n        {\"key\": \"programming_language\", \"match\": {\"value\": \"python\"}},\n        {\"key\": \"git_project_name\", \"match\": {\"value\": \"my-project\"}}\n    ]\n}\n```\n\nResult Format:\n```json\n{\n    \"results\": [\n        {\n            \"score\": 0.95,\n            \"file_path\": \"/path/to/file.py:123\",\n            \"content\": \"function implementation...\",\n            \"metadata\": {\n                \"programming_language\": \"python\",\n                \"git_project_name\": \"my-project\",\n                \"chunk_index\": 5\n            },\n            \"collection\": \"outstar-source-code\"\n        }\n    ]\n}\n```\n\nCLI Wrapper:\n```bash\narcaneum search \"authentication patterns\" \\\n  --collection CodeLibrary \\\n  --limit 5 \\\n  --filter language=python\n```\n\nFuture: Hybrid Search\n- Integration with MeiliSearch for phrase matching\n- Reciprocal Rank Fusion (RRF) algorithm\n- Configurable weights (70% semantic, 30% full-text)","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.719691-07:00","updated_at":"2025-10-20T07:41:10.719691-07:00","dependencies":[{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.736893-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.738007-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.738969-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T07:41:10.739715-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-8","title":"Research embedding model flexibility and dynamic caching","description":"Investigate how to support multiple embedding models dynamically with caching. Research FastEmbed, sentence-transformers, and other libraries for on-demand model downloading and caching strategies.","notes":"Research completed. FastEmbed recommended for lightweight, self-contained CLI with automatic caching. Supports stella, bge-large, jina-code models with 1024/768 dimensions.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.720456-07:00","updated_at":"2025-10-20T07:41:10.720456-07:00","closed_at":"2025-10-19T14:30:51.232555-07:00","dependencies":[{"issue_id":"arcaneum-8","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.740757-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-9","title":"Research model token length constraints and optimal chunking","description":"Investigate training token run lengths for embedding models (stella, modernbert, bge-large, jina-code). Determine optimal chunking strategies for Qdrant based on model constraints. Compare with ChromaDB learnings.","notes":"Research completed. Token limits: stella 512-1024, modernbert 8192, bge-large 512, jina-code 8192. Chunk sizes: 460-920 tokens with 10-20% overlap. Store-specific adjustments documented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.721284-07:00","updated_at":"2025-10-20T07:41:10.721284-07:00","closed_at":"2025-10-19T14:30:51.294453-07:00","dependencies":[{"issue_id":"arcaneum-9","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.741543-07:00","created_by":"auto-import"}]}
