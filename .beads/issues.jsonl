{"id":"arcaneum-1","title":"Create Claude Code marketplace project structure","description":"Create the foundational directory structure and packaging setup for the Arcaneum Claude Code marketplace project. This establishes where all code lives and how components are packaged/distributed.\n\nDeliverables:\n- pyproject.toml with modern Python packaging\n- Directory structure: src/, plugins/, docs/, tests/\n- MCP server plugin scaffold\n- Installation method via uvx or pip\n- README with quick start guide\n\nThis is the foundation that all other RDRs build upon.","design":"Directory Structure:\n```\narcaneum/\n├── pyproject.toml          # Modern packaging\n├── README.md               # Quick start\n├── src/\n│   └── arcaneum/\n│       ├── __init__.py\n│       ├── server/         # MCP server core\n│       ├── indexing/       # Bulk upload logic\n│       └── search/         # Search utilities\n├── plugins/\n│   ├── qdrant-server/      # Server management plugin\n│   ├── qdrant-indexer/     # Bulk upload plugin\n│   └── qdrant-search/      # Search plugin\n├── tests/\n└── docs/\n\nInstallation:\n- uvx install arcaneum\n- pip install arcaneum\n\nMCP Plugin Discovery:\n- Each plugin has its own pyproject.toml\n- Plugins register via entry points\n- Claude Code discovers via marketplace registry","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.685704-07:00","updated_at":"2025-10-20T07:41:10.685704-07:00","closed_at":"2025-10-19T10:23:12.252341-07:00"}
{"id":"arcaneum-10","title":"Review Chroma embedding references and validate for Qdrant","description":"Review chroma-embedded/upload.sh and outstar-rag-requirements.md for embedding model usage patterns. Validate if these approaches apply to Qdrant or need adaptation.","notes":"Review completed. ChromaDB patterns transfer directly to Qdrant with adaptations: increase batch size to 100-200, client-side embeddings, same chunking strategies. FastEmbed equivalence needs validation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.68751-07:00","updated_at":"2025-10-20T07:41:10.68751-07:00","closed_at":"2025-10-19T14:30:51.356705-07:00","dependencies":[{"issue_id":"arcaneum-10","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.722364-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-11","title":"Research Qdrant client-side embedding capabilities","description":"Review qdrant-client Python library source code for embedding model integration, FastEmbed support, and collection configuration options. Document features and APIs.","notes":"Research completed. Qdrant client has deep FastEmbed integration. Collection creation via create_collection() with VectorParams. Named vectors supported. HNSW config: m=16, ef_construct=100 defaults.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.688474-07:00","updated_at":"2025-10-20T07:41:10.688474-07:00","closed_at":"2025-10-19T14:30:51.424705-07:00","dependencies":[{"issue_id":"arcaneum-11","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.723279-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-12","title":"Research opensource projects for collection management","description":"Search for existing opensource tools/libraries that manage Qdrant collections with embedding models. Evaluate as replacement or inspiration (mcp-server-qdrant, qdrant-haystack, etc).","notes":"Research completed. Recommendation: Build from scratch using qdrant-client+FastEmbed (Apache 2.0). Avoid GPL-3.0 tools. Inspiration from analogrithems/qdrant-cli structure, mcp-server-qdrant patterns. Bundle QdrantUI as companion.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.689348-07:00","updated_at":"2025-10-20T07:41:10.689348-07:00","closed_at":"2025-10-19T14:30:51.484132-07:00","dependencies":[{"issue_id":"arcaneum-12","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.724072-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-13","title":"Design init vs create-collection CLI architecture","description":"Design CLI structure with init (one-time setup) and create-collection commands. Determine what belongs in init (model cache setup, server validation) vs collection creation.","notes":"Design completed. CLI structure: init (server health check, model cache setup, server config validation) and collection commands (create, list, delete, info). Collection creation includes named vectors setup, HNSW config, payload indexes. Model management: list, download, cache-info commands.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.690311-07:00","updated_at":"2025-10-20T07:41:10.690311-07:00","closed_at":"2025-10-19T14:31:31.235733-07:00","dependencies":[{"issue_id":"arcaneum-13","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.724801-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-14","title":"Write RDR-003 document","description":"Create doc/rdr/RDR-003-collection-creation.md with all research findings, design decisions, and implementation guidance.","notes":"RDR-003 document written with all sections complete. Includes metadata, problem statement, research findings, technical design, CLI architecture, configuration system, implementation examples, alternatives, trade-offs, implementation plan, validation, and references.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.691206-07:00","updated_at":"2025-10-20T07:41:10.691206-07:00","closed_at":"2025-10-19T14:42:26.150428-07:00","dependencies":[{"issue_id":"arcaneum-14","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.725549-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-15","title":"Review prior RDRs for context and design patterns","description":"Read all existing RDR files in doc/rdr/ to understand established patterns, design decisions, and technical approaches that should influence the PDF indexing RDR.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.692233-07:00","updated_at":"2025-10-20T07:41:10.692233-07:00","closed_at":"2025-10-19T17:01:39.984235-07:00"}
{"id":"arcaneum-16","title":"Review chroma-embedded/upload.sh for PDF processing patterns","description":"Analyze the referenced chroma-embedded/upload.sh script, specifically lines 1372-1522 (PDF extraction) and lines 269-324 (token-optimized chunking) to understand existing implementation patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.69306-07:00","updated_at":"2025-10-20T07:41:10.69306-07:00","closed_at":"2025-10-19T17:03:49.096247-07:00"}
{"id":"arcaneum-17","title":"Review outstar-rag-requirements.md for PDF requirements","description":"Read outstar-rag-requirements.md lines 136-167 to understand the specific PDF processing requirements for this project.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.693879-07:00","updated_at":"2025-10-20T07:41:10.693879-07:00","closed_at":"2025-10-19T17:04:23.029999-07:00"}
{"id":"arcaneum-18","title":"Research PyMuPDF (fitz) capabilities and limitations","description":"Deep dive into PyMuPDF open source code to understand text extraction capabilities, table handling, performance characteristics, and edge cases.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.694701-07:00","updated_at":"2025-10-20T07:41:10.694701-07:00","closed_at":"2025-10-19T17:10:46.570322-07:00"}
{"id":"arcaneum-19","title":"Research pdfplumber capabilities and limitations","description":"Deep dive into pdfplumber open source code to understand text extraction capabilities, table handling, performance characteristics, and when it excels vs PyMuPDF.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.695536-07:00","updated_at":"2025-10-20T07:41:10.695536-07:00","closed_at":"2025-10-19T17:10:46.614683-07:00"}
{"id":"arcaneum-2","title":"RDR for running Qdrant server with embedding model configuration","description":"Create an RDR that defines how to run and configure a Qdrant server instance for the Arcaneum marketplace. Must address Docker vs local binary, port configuration, volume persistence, and embedding model setup.\n\nKey Design Questions:\n- Docker official image or custom build?\n- How to configure FastEmbed for client-side embeddings?\n- Volume mounting strategy for data persistence\n- Multi-model support per collection\n- gRPC vs REST API preference\n\nReferences:\n- /Users/cwensel/sandbox/outstar/research/qdrant-local/server.sh\n- outstar-rag-requirements.md sections on Qdrant (lines 82-94)","design":"Initial Design Direction (to be refined in RDR):\n\nDocker Setup:\n- Official qdrant/qdrant:latest image\n- Port 6333 (REST), 6334 (gRPC)\n- Volume: ./qdrant_storage:/qdrant/storage\n- Health checks via /health endpoint\n\nEmbedding Strategy:\n- Client-side with FastEmbed (no server modification)\n- Models: stella (1024d), modernbert (1024d), bge-large (1024d), jina-code (768d)\n- Model cache: ./models_cache/ mounted\n\nCollection Architecture:\n- One collection per (document-type, embedding-model) pair\n- Example: outstar-source-code-jinacode, outstar-pdf-stella\n- Metadata stores embedding model name for validation\n\nServer Management:\n- Start/stop/restart commands\n- Log access\n- Resource limits (4GB RAM default)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.696384-07:00","updated_at":"2025-10-20T07:41:10.696384-07:00","closed_at":"2025-10-19T10:42:09.54722-07:00","dependencies":[{"issue_id":"arcaneum-2","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.726613-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-20","title":"Research Tesseract OCR capabilities and integration","description":"Investigate Tesseract OCR system dependencies, language support, accuracy characteristics, confidence scoring, and Python integration patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.697254-07:00","updated_at":"2025-10-20T07:41:10.697254-07:00","closed_at":"2025-10-19T17:10:46.656407-07:00"}
{"id":"arcaneum-21","title":"Research EasyOCR capabilities and integration","description":"Investigate EasyOCR pure Python implementation, language support, accuracy vs Tesseract, performance characteristics, and integration patterns.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.698059-07:00","updated_at":"2025-10-20T07:41:10.698059-07:00","closed_at":"2025-10-19T17:10:46.702429-07:00"}
{"id":"arcaneum-22","title":"Research embedding model token limits and chunking strategies","description":"Investigate token limits for stella, modernbert, and bge-large models. Understand optimal chunking strategies, overlap recommendations, and char-to-token ratios.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.699076-07:00","updated_at":"2025-10-20T07:41:10.699076-07:00","closed_at":"2025-10-19T17:16:49.241549-07:00"}
{"id":"arcaneum-23","title":"Research Qdrant batch upload capabilities and best practices","description":"Investigate Qdrant's batch upload API, optimal batch sizes, error handling, retry strategies, and performance characteristics for large-scale indexing.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.699932-07:00","updated_at":"2025-10-20T07:41:10.699932-07:00","closed_at":"2025-10-19T17:16:49.284168-07:00"}
{"id":"arcaneum-24","title":"Research chroma-embedded git handling patterns","description":"Analyze chroma-embedded/upload.sh git handling (lines 373-433, 846-976) to understand:\n- Git project discovery with find .git and depth control\n- Metadata extraction (commit_hash, remote_url, branch, project_name)\n- Change detection logic comparing stored vs current commit\n- Bulk deletion of changed projects\n- Integration with .gitignore via 'git ls-files'\n\nReference: /Users/cwensel/sandbox/outstar/research/chroma-embedded/upload.sh","notes":"Research Complete: Analyzed chroma-embedded/upload.sh git handling (lines 373-433, 846-976).\n\nKey Findings:\n- Git discovery uses find .git with depth control (depth+1 for maxdepth calculation)\n- Metadata extraction: commit_hash (git rev-parse HEAD), remote_url (git remote get-url origin), branch (git branch --show-current), project_name (basename)\n- Change detection: Compare stored vs current commit hash, trigger bulk deletion if changed\n- Bulk deletion strategy: 2-tier batching (1000 retrieve, 100 delete) with offset pagination\n- .gitignore integration: Uses 'git ls-files' to respect ignore patterns, converts relative to absolute paths\n- Edge cases handled: detached HEAD (fallback to \"unknown\"), missing remote (fallback), shallow clones\n\nPatterns for Qdrant adaptation:\n- Use same git discovery logic\n- Implement filter-based deletion (faster than ChromaDB's ID-based)\n- Maintain commit hash comparison for change detection\n- Support depth control via CLI parameter","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.700839-07:00","updated_at":"2025-10-20T07:41:10.700839-07:00","closed_at":"2025-10-19T21:14:08.502224-07:00"}
{"id":"arcaneum-25","title":"Research ASTChunk library for multi-language code chunking","description":"Deep research on ASTChunk library capabilities:\n- Verify support for 15+ languages (Python, Java, JS/TS, C#, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift)\n- AST-aware chunking strategies preserving function/class boundaries\n- Token sizing and safety buffers\n- Fallback mechanisms when AST parsing fails\n- Integration patterns and configuration options\n\nUse opensource-code-explorer agent to find real-world usage examples.","notes":"Research Complete: Deep analysis of ASTChunk library and alternatives.\n\nKey Findings:\n- ASTChunk supports ONLY 4 languages (Python, Java, C#, TypeScript) - insufficient for requirements\n- Missing 11 required languages: JavaScript, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift\n- No built-in fallback when AST parsing fails\n- Uses cAST algorithm: recursive node splitting + greedy merging\n- Token sizing via non-whitespace character count (not actual tokens)\n\nCRITICAL DECISION: Cannot use ASTChunk alone\n\nRecommended Alternative: tree-sitter-language-pack\n- Supports 165+ languages (covers all 15+ requirements)\n- LlamaIndex CodeSplitter provides mature implementation\n- Built-in error handling and fallbacks\n- Integration pattern: get_parser(language) → AST-based chunking → fallback to line-based\n\nReal-world validation:\n- ChunkHound uses cAST with 24 languages\n- CodeSearchNet uses tree-sitter for 6 languages\n- Code-splitter (Rust) supports all tree-sitter languages\n\nImplementation: Use tree-sitter-language-pack as foundation, not ASTChunk","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.701685-07:00","updated_at":"2025-10-20T07:41:10.701685-07:00","closed_at":"2025-10-19T21:14:08.566659-07:00"}
{"id":"arcaneum-26","title":"Study qdrant-local source code handling vs chroma-embedded","description":"Compare qdrant-local and chroma-embedded approaches to source code indexing:\n- Identify Qdrant-specific optimizations vs ChromaDB patterns\n- Metadata schema differences\n- Batch upload strategies for code vs PDFs\n- Performance characteristics\n\nReference: /Users/cwensel/sandbox/outstar/research/qdrant-local/","notes":"Research Complete: Comprehensive comparison of Qdrant-local vs chroma-embedded.\n\nKey Differences:\n- Embedding: Qdrant uses client-side (FastEmbed), ChromaDB uses server-side\n- Batch sizes: Qdrant handles 100-200 chunks, ChromaDB limited to 25-50 (HTTP payload limits)\n- Deletion: Qdrant filter-based (40-100x faster), ChromaDB requires ID retrieval first\n- Communication: Qdrant supports gRPC + REST, ChromaDB HTTP only\n\nSource Code Indexing Specifics:\n- Both reduce chunk size by 60 tokens for code (better AST parsing)\n- Both reduce overlap to 50% for code\n- Same git awareness patterns (ls-files, commit tracking)\n- Same AST chunking integration (when available)\n\nQdrant Optimizations to Leverage:\n1. Use filter-based deletion for bulk operations (50-500ms vs 20-50s)\n2. Increase batch sizes to 100-200 (no HTTP limits)\n3. Use gRPC for 2-3x faster uploads\n4. Hierarchical metadata grouping for efficient filtering\n5. Native metadata filtering (no client-side deduplication needed)\n\nPerformance: Qdrant's client-side embeddings + native filtering make it superior for complex multi-project indexing workflows.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.7024-07:00","updated_at":"2025-10-20T07:41:10.7024-07:00","closed_at":"2025-10-19T21:14:08.633835-07:00"}
{"id":"arcaneum-27","title":"Research jina-code embedding model characteristics","description":"Investigate jina-code (jina-embeddings-v3) for code embeddings:\n- Token limits and context window\n- Optimal chunk sizes for code (400 tokens target?)\n- Character-to-token ratios for various programming languages\n- Code-specific optimizations vs general embeddings\n- Comparison with stella/modernbert for code workloads\n\nUse web search and opensource research for jina-code documentation.","notes":"Research Complete: Jina embedding models for code analyzed.\n\nKey Findings:\nTHREE Jina models for code (not just one):\n1. jina-embeddings-v2-base-code: 161M params, 8K context, 768D, 0.7753 accuracy, Apache 2.0\n2. jina-embeddings-v3: 570M params, 8K context, 1024D, 0.7564 accuracy (WORSE for code), CC BY-NC 4.0\n3. jina-code-embeddings-0.5b/1.5b: NEW models, 32K context, 79% accuracy, last-token pooling\n\nRECOMMENDATION: Use jina-code-embeddings-1.5b\n- Best performance: 79.04% avg, 92.37% StackOverflow, 86.45% CodeSearchNet\n- 32K context window (4x larger, can embed entire files)\n- 1536 dimensions with Matryoshka support\n- Optimized on Qwen2.5-Coder foundation\n- Matches voyage-code-3 performance\n\nChunking Strategy:\n- For 32K context: Embed entire files (most fit), use 2K-4K chunks for large files\n- For 8K context (v2-base-code): 400-512 tokens per chunk\n- Character-to-token ratio: Conservative 3.5 chars/token for code\n- 400 tokens ≈ 1,200-1,400 characters\n\nFastEmbed Integration:\n- v2-base-code: Fully supported\n- v3: Supported with task adapters (CC BY-NC license restriction)\n- code-embeddings: Status unknown, may need manual integration\n\nAlternative: Use v2-base-code (smaller, proven, Apache 2.0) if 1.5B model unavailable in FastEmbed","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.703121-07:00","updated_at":"2025-10-20T07:41:10.703121-07:00","closed_at":"2025-10-19T21:14:08.704065-07:00"}
{"id":"arcaneum-28","title":"Explore open source code indexing tools and patterns","description":"Research production code indexing systems for inspiration:\n- GitHub semantic code search implementation patterns\n- Sourcegraph indexing strategies\n- CodeSearchNet dataset approaches\n- tree-sitter usage for language-agnostic parsing\n- Other AST-based chunking libraries\n\nUse opensource-code-explorer agent to find relevant projects.","notes":"Research Complete: Analyzed 20+ open source code indexing tools.\n\nProjects Cloned \u0026 Analyzed (in /Users/cwensel/sandbox/thirdparty/):\n- ASTChunk: cAST algorithm implementation (4 languages)\n- ChunkHound: MCP integration, 24 languages, multi-hop search\n- Code-Splitter: Rust crate, tree-sitter, multiple tokenizers\n- Chonkie: Ultra-light RAG, 56 languages, pipeline-based\n- CodeSearchNet: GitHub dataset, 6M methods, tree-sitter tokenization\n- Semantic-Code-Search: CLI tool, local-first, 15 languages\n- SeaGOAT: ChromaDB integration, regex + semantic hybrid\n- CodeQAI: FAISS integration, git-aware sync\n- SCIP (Sourcegraph): Language Server Index Format, 10x faster than LSIF\n- Code2Vec: AST path-based embeddings\n- tree-sitter-language-pack: 165+ languages\n- LlamaIndex CodeSplitter: Mature AST chunking with fallbacks\n\nKey Techniques Identified:\n1. cAST Algorithm: Recursive splitting + greedy merging (proven 4.3 point gain)\n2. Tree-sitter: Dominant parser (165+ languages)\n3. Hybrid Search: Combine regex + semantic for better results\n4. MCP Integration: Standard protocol for AI assistants\n5. Real-time Indexing: File watchers for automatic updates\n6. Multi-hop Search: Follow code relationships\n\nRecommendations for Arcaneum:\n- Adopt cAST algorithm via tree-sitter-language-pack\n- Implement MCP protocol for Claude integration\n- Support hybrid search (semantic + full-text)\n- Add real-time indexing with file watching\n- Consider multi-hop exploration features","status":"closed","priority":1,"issue_type":"task","assignee":"assistant","created_at":"2025-10-20T07:41:10.703958-07:00","updated_at":"2025-10-20T07:41:10.703958-07:00","closed_at":"2025-10-19T21:14:08.792668-07:00"}
{"id":"arcaneum-29","title":"Research git metadata extraction best practices","description":"Study git metadata extraction patterns:\n- Commit hash tracking (short vs full hashes)\n- Remote URL handling (origin, multiple remotes)\n- Branch detection and tracking\n- Project name derivation strategies\n- Handling detached HEAD, shallow clones, submodules\n- Error handling for corrupt/incomplete git repos\n\nReference chroma-embedded patterns and research git best practices.","notes":"Research Complete: Git metadata extraction best practices analyzed.\n\nKey Recommendations:\n1. Commit Hash: Store FULL hash (40 chars), display 12-char abbreviated\n   - Enables cryptographic verification\n   - Supports git object inspection\n   - Better cross-system compatibility\n\n2. Remote URL: Priority order - origin \u003e upstream \u003e first remote\n   - SECURITY: Strip credentials (https://user:pass@host → https://host)\n   - Handle multiple remotes gracefully\n   - Sanitize before storage\n\n3. Branch Detection: Robust fallback chain\n   - git branch --show-current (primary)\n   - git describe --tags --exact-match (detached HEAD on tag)\n   - git name-rev --name-only HEAD (detached HEAD fallback)\n   - \"(detached-\u003cshort-hash\u003e)\" (last resort)\n\n4. Project Name: Multi-source derivation\n   - Extract from remote URL (priority)\n   - Use git config user.projectname (if set)\n   - Fallback to basename (directory name)\n   - Normalize: remove .git suffix, sanitize special chars\n\n5. Submodules: Track separately\n   - Check .gitmodules file existence\n   - Record submodule commits for complete tracking\n   - Option to skip for faster indexing\n\n6. Error Handling:\n   - Detect shallow clones (.git/shallow file)\n   - Handle corrupt repos (git fsck)\n   - Timeout protection (5s limit per git command)\n   - Use 'git -C' instead of 'cd' (safer for parallel ops)\n\nEdge Cases Covered:\n- Detached HEAD, shallow clones, submodules, missing remotes, corrupt repos, empty repos, multiple remotes, credentials in URLs","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.704822-07:00","updated_at":"2025-10-20T07:41:10.704822-07:00","closed_at":"2025-10-19T21:14:08.869209-07:00"}
{"id":"arcaneum-3","title":"RDR for CLI/plugin to create collections in Qdrant with embeddings","description":"Create an RDR for a CLI tool/MCP plugin that creates Qdrant collections with proper embedding model configuration. Must ensure model consistency across indexing and querying.\n\nKey Design Questions:\n- Collection naming convention (prefix + document type + model?)\n- How to validate model dimensions match collection vector size?\n- Metadata schema versioning strategy\n- HNSW index configuration (m, ef_construct)\n- Distance metric selection (cosine recommended)\n\nReferences:\n- outstar-rag-requirements.md lines 82-94, 213-237\n- qdrant-local collection creation patterns","design":"Initial Design Direction:\n\nCLI Interface:\n```bash\narcaneum collection create NAME \\\n  --model stella \\\n  --doc-type source-code \\\n  --distance cosine \\\n  --hnsw-m 16 \\\n  --hnsw-ef 100\n```\n\nCollection Metadata Schema:\n- embedding_model: \"stella\" | \"modernbert\" | \"bge-large\" | \"jina-code\"\n- vector_dimensions: 1024 | 768\n- doc_type: \"source-code\" | \"pdf\" | \"markdown\"\n- created_at: ISO timestamp\n- schema_version: \"1.0\"\n\nValidation:\n- Check model dimensions match vector_size\n- Prevent duplicate collection names\n- Verify server connectivity before creation\n\nModel-Dimension Mapping:\n- stella: 1024\n- modernbert: 1024  \n- bge-large: 1024\n- jina-code: 768\n\nHNSW Defaults:\n- m=16 (connections per layer)\n- ef_construct=100 (construction quality)\n- Disable during bulk upload (m=0), enable after","notes":"RDR-003 completed. Document created at doc/rdr/RDR-003-collection-creation.md with comprehensive research findings, CLI-driven configuration design, FastEmbed integration, named vectors architecture, and implementation plan. All 6 research tracks completed and documented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.705579-07:00","updated_at":"2025-10-20T07:41:10.705579-07:00","closed_at":"2025-10-19T14:42:26.102009-07:00","external_ref":"doc/rdr/RDR-003-collection-creation.md","dependencies":[{"issue_id":"arcaneum-3","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.727383-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-3","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.728128-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-30","title":"Research change detection and deduplication strategies","description":"Investigate change detection approaches:\n- Bulk delete vs incremental update trade-offs\n- SQLite checkpoint DB schema design for tracking\n- File hash deduplication (SHA-256 patterns)\n- Resume/checkpoint strategies for interrupted indexing\n- Performance implications of bulk deletion in Qdrant\n\nCompare chroma-embedded approach with Qdrant capabilities.","notes":"Research Complete: Change detection and deduplication strategies analyzed.\n\nRECOMMENDATION: Hybrid Approach (Bulk Delete + File Hash)\n\nTrade-off Analysis:\n- Bulk Delete: Simple, atomic, consistent - Best for project-level changes\n- Incremental: Optimal for small changes - Complex, partial failure risk\n- Hybrid: Combines both strengths\n\nMulti-Level Detection Strategy:\n1. Level 1 (Git Commit): Compare commit hashes - Triggers bulk delete\n2. Level 2 (File Hash): SHA-256 content hash - Triggers selective reindex\n3. Level 3 (Timestamp): File mtime check - Optimization (skip hash if old)\n\nSQLite Checkpoint DB Schema:\n- batches table: Track upload progress, enable resume\n- file_index table: SHA-256 hash deduplication, status tracking\n- git_projects table: Commit hash tracking, change detection\n- transactions table: Audit trail, debugging\n\nQdrant Performance:\n- Filter-based deletion: 50-500ms (40-100x faster than ChromaDB ID-based)\n- Optimal batch size: 100-200 chunks for upload\n- HNSW index: Handles deletions efficiently (bitmap marking)\n- No index rebuild needed on deletion\n\nImplementation:\n- Use filter-based delete for bulk operations\n- Cache commit hashes in SQLite (avoid repeated git commands)\n- Batch file hash checks (100 at a time)\n- Mark stale records (recovery option)\n- Update checkpoint every 100 files (resume granularity)\n\nPerformance Optimizations:\n- Filter deletion: O(1) vs O(n) for ID-based\n- Parallel workers: Process different files independently\n- Hash computation: 0.1-1s per file (optimize with mtime check first)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.706305-07:00","updated_at":"2025-10-20T07:41:10.706305-07:00","closed_at":"2025-10-19T21:14:09.566866-07:00"}
{"id":"arcaneum-31","title":"Research non-git directory indexing fallback patterns","description":"Study fallback strategies for non-git directories:\n- Directory tree traversal patterns\n- Metadata schema without git information\n- File filtering without .gitignore\n- Change detection via file modification times\n- Handling mixed git/non-git directory trees\n\nEnsure feature parity for users indexing non-git code.","notes":"Research Complete: Non-git directory indexing fallback patterns.\n\nKey Findings:\n- Chroma-embedded ALREADY has fallback (lines 483-489) - uses 'find' with extensions\n- Fallback automatically activates when no git projects found\n- Can achieve 95%+ feature parity with enhancements\n\nRecommended Enhancements:\n\n1. Metadata Schema Additions:\n   - index_mode: \"git\" | \"directory\"\n   - directory_root: Equivalent to git_project_root\n   - file_modified_timestamp: Change detection without commit hash\n   - file_content_hash: SHA-256 for content-based deduplication\n\n2. File Filtering (without .gitignore):\n   - Configurable ignore patterns\n   - Defaults: node_modules, .venv, __pycache__, .git, build/, dist/, .DS_Store, *.pyc, *.o\n   - Size-based exclusions (skip files \u003e 10MB)\n   - Extension allowlist for safety\n\n3. Change Detection Strategy:\n   - Fast path: Check file mtime \u003c last_index_time\n   - Accurate path: Compute SHA-256 hash, compare with stored\n   - Hybrid: Use mtime to filter candidates, hash to confirm\n\n4. Mixed Git/Non-Git Handling:\n   - Classify each subdirectory independently\n   - Git repos: Use git ls-files + commit tracking\n   - Non-git dirs: Use find + ignore patterns\n   - Graceful mode switching per directory\n\n5. Directory Traversal:\n   - Use find with -type f for files only\n   - Handle symlinks: -follow (include) or default (skip)\n   - Respect hidden directories: exclude .* unless explicitly included\n\nFeature Parity Assessment:\n- Git mode: O(1) change detection via commit hash, fast\n- Directory mode: O(n) change detection via file timestamps, reliable\n\nImplementation Phases:\n- Phase 1 (minimal): Add index_mode, directory_root tracking\n- Phase 2 (enhanced): Timestamp + hash hybrid change detection\n- Phase 3 (production): Configurable patterns, size limits, symlink handling","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.707047-07:00","updated_at":"2025-10-20T07:41:10.707047-07:00","closed_at":"2025-10-19T21:14:10.255699-07:00"}
{"id":"arcaneum-32","title":"Update RDR-005 metadata schema with composite git_project_identifier","description":"Merge multi-branch addendum into RDR-005 main document.\n\nUpdate metadata schema section to use composite identifier:\n- Add git_project_identifier = f\"{project_name}#{branch}\"\n- Keep git_project_name and git_branch as separate fields for filtering\n- Remove backward compatibility discussion\n- Update code examples to use identifier\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (lines ~620-665)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.707827-07:00","updated_at":"2025-10-20T07:41:10.707827-07:00","closed_at":"2025-10-19T21:35:48.115774-07:00"}
{"id":"arcaneum-33","title":"Update RDR-005 SQLite checkpoint schema for multi-branch","description":"Update checkpoint DB schema to use composite primary key.\n\nChange git_projects table from:\n- PRIMARY KEY (project_name)\nTo:\n- PRIMARY KEY (project_name, branch)\n\nUpdate all schema documentation and code examples.\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"CREATE TABLE git_projects\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.70859-07:00","updated_at":"2025-10-20T07:41:10.70859-07:00","closed_at":"2025-10-19T21:35:48.172558-07:00"}
{"id":"arcaneum-34","title":"Update RDR-005 change detection logic for branch-aware checking","description":"Update ChangeDetector code examples to check (project, branch) tuples.\n\nChanges:\n- Update should_reindex() to accept branch parameter\n- Query checkpoint DB by (project_name, branch) not just project_name\n- Add NEW_BRANCH to ChangeType enum\n- Update all code examples\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"ChangeDetector\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.709353-07:00","updated_at":"2025-10-20T07:41:10.709353-07:00","closed_at":"2025-10-19T21:35:48.230415-07:00"}
{"id":"arcaneum-35","title":"Update RDR-005 deletion logic to use git_project_identifier","description":"Update QdrantIndexer deletion methods to filter by composite identifier.\n\nChanges:\n- Rename delete_project_chunks() to delete_branch_chunks()\n- Update filter to use git_project_identifier field\n- Update all code examples and references\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"delete_project_chunks\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.710111-07:00","updated_at":"2025-10-20T07:41:10.710111-07:00","closed_at":"2025-10-19T21:35:48.289423-07:00"}
{"id":"arcaneum-36","title":"Update RDR-005 main pipeline to generate and use composite identifiers","description":"Update SourceCodeIndexer pipeline code to create and use git_project_identifier.\n\nChanges:\n- Generate identifier = f\"{project_name}#{branch}\" for each repo\n- Pass identifier through processing pipeline\n- Update checkpoint recording calls\n- Update logging to show branch context\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (search for \"SourceCodeIndexer\")","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.710942-07:00","updated_at":"2025-10-20T07:41:10.710942-07:00","closed_at":"2025-10-19T21:35:48.351113-07:00"}
{"id":"arcaneum-37","title":"Add multi-branch usage examples to RDR-005","description":"Add usage examples showing multi-branch workflow.\n\nExamples to add:\n1. Index directory with repos on different branches\n2. User switches branch and re-indexes (new branch added)\n3. User commits on branch (only that branch re-indexed)\n4. Branch comparison queries\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (after Implementation Example section)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.711638-07:00","updated_at":"2025-10-20T07:41:10.711638-07:00","closed_at":"2025-10-19T21:35:48.668912-07:00"}
{"id":"arcaneum-38","title":"Update RDR-005 test scenarios for multi-branch support","description":"Update test scenarios in Validation section for multi-branch.\n\nAdd scenarios:\n- Multiple branches of same repo indexed\n- Update one branch (others untouched)\n- Branch comparison query\n- Resume interrupted multi-branch indexing\n\nUpdate existing scenarios to mention branch context.\n\nLocation: doc/rdr/RDR-005-source-code-indexing.md (Validation section)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.712371-07:00","updated_at":"2025-10-20T07:41:10.712371-07:00","closed_at":"2025-10-19T21:35:49.001323-07:00"}
{"id":"arcaneum-39","title":"Remove non-git directory support from RDR-005","description":"Simplify RDR-005 by removing non-git directory fallback support.\n\nRemove/update:\n- DirectoryIndexer class and code examples\n- Non-git metadata fields (directory_root, file_modified_timestamp)\n- index_mode field (always \"git\" now)\n- Test Scenario 4 (non-git directory)\n- Test Scenario 5 (mixed git/non-git)\n- Step 5 from implementation plan\n- arcaneum-31 reference from research findings\n- All \"Non-Git\" sections\n\nResult: Cleaner, git-only design (15% reduction in complexity)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.71307-07:00","updated_at":"2025-10-20T07:41:10.71307-07:00","closed_at":"2025-10-19T21:35:49.346411-07:00"}
{"id":"arcaneum-4","title":"RDR for bulk indexing PDF files with OCR support","description":"Create an RDR for bulk PDF indexing with OCR support, adapted from chroma-embedded/upload.sh. Must handle text PDFs, image PDFs, mixed PDFs, and optimize chunking for embedding models.\n\nKey Design Questions:\n- PyMuPDF vs pdfplumber for text extraction?\n- Tesseract vs EasyOCR for image PDFs?\n- When to trigger OCR (threshold for \"no text\")?\n- Chunking strategy - token-aware with model-specific sizing?\n- Batch upload size for Qdrant (100-200 chunks)?\n- Error handling for corrupt PDFs\n\nReferences:\n- chroma-embedded/upload.sh lines 1372-1522 (PDF extraction)\n- chroma-embedded/upload.sh lines 269-324 (token-optimized chunking)\n- outstar-rag-requirements.md lines 136-167 (PDF requirements)","design":"Initial Design Direction:\n\nText Extraction:\n- Primary: PyMuPDF (fitz) - 10x faster, low memory\n- Fallback: pdfplumber for complex tables\n- Trigger OCR if extracted text \u003c 100 chars\n\nOCR Strategy:\n- Default: Tesseract (faster, system dep)\n- Alternative: EasyOCR (pure Python, no system deps)\n- Multi-language support via --ocr-language flag\n- 2x image scaling for better accuracy\n- Confidence scoring to identify poor extractions\n\nChunking:\n- Model-specific token limits:\n  - stella: 460 tokens (512 limit - 10% margin)\n  - modernbert: 920 tokens (1024 limit - 10% margin)\n  - bge-large: 460 tokens (512 limit - 10% margin)\n- Char-to-token ratios per model (stella: 3.2, modernbert: 3.4)\n- 10% overlap between chunks\n\nMetadata Schema:\n- file_path, filename, file_size\n- text_extraction_method: \"pymupdf\" | \"ocr_tesseract\" | \"ocr_easyocr\"\n- is_image_pdf: boolean\n- ocr_confidence: float (0-100)\n- chunk_index, chunk_count\n- embedding_model, store_type: \"pdf\"\n\nBatch Upload:\n- 100-200 chunks per batch (Qdrant handles larger than ChromaDB)\n- Exponential backoff on failures (1s, 5s, 25s)\n- Progress reporting per file","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.713763-07:00","updated_at":"2025-10-20T07:41:10.713763-07:00","closed_at":"2025-10-19T18:03:55.881662-07:00","dependencies":[{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.728846-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.72954-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-4","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.730215-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-40","title":"Delete RDR-005 addendum file after merging","description":"Delete doc/rdr/RDR-005-ADDENDUM-multi-branch.md after content is merged into main RDR-005.\n\nThis keeps documentation concise with single source of truth.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-10-20T07:41:10.714493-07:00","updated_at":"2025-10-20T07:41:10.714493-07:00","closed_at":"2025-10-19T21:35:49.683439-07:00"}
{"id":"arcaneum-41","title":"Update RDR-005 to use Qdrant metadata-based sync (like RDR-04)","description":"Change RDR-005 from SQLite-based change detection to Qdrant metadata queries for consistency with RDR-04.\n\nCurrent problem: SQLite checkpoint is source of truth for change detection, can get out of sync if chunks manually deleted from Qdrant.\n\nSolution: Follow RDR-04 pattern:\n1. Query Qdrant for (git_project_identifier, git_commit_hash) pairs\n2. Use Qdrant metadata as source of truth for change detection\n3. Keep SQLite checkpoint ONLY for crash recovery (batch resumability)\n\nThis provides:\n- Single source of truth (Qdrant)\n- Architectural consistency with RDR-04\n- Handles manual deletions correctly","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.715273-07:00","updated_at":"2025-10-20T07:41:10.715273-07:00","closed_at":"2025-10-19T21:42:12.637751-07:00"}
{"id":"arcaneum-42","title":"Review and fix RDR-005 inconsistencies","description":"Comprehensive review of RDR-005 to identify and fix:\n\n1. Inconsistencies between sections\n2. Outdated content (mentions of 3-level change detection that now doesn't exist)\n3. References to removed features (file hash computation, directory mode)\n4. Conflicting information about SQLite checkpoint role\n5. Architecture diagram that doesn't match current design\n6. Approach section (#6) mentions \"commit hash → file hash → mtime\" but we removed file hash\n7. Negative consequences mention \"SQLite Schema Complexity\" but we simplified it\n8. Performance tests mention \"Compare git mode vs directory mode\" but we removed directory mode\n9. Risk mitigation mentions \"graceful degradation to directory mode\" but we're git-only\n\nNeed to ensure document is internally consistent with the metadata-based sync approach.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.715986-07:00","updated_at":"2025-10-20T07:41:10.715986-07:00","closed_at":"2025-10-19T21:51:37.022661-07:00"}
{"id":"arcaneum-43","title":"Evaluate if SQLite checkpoint needed for RDR-005","description":"Question: Do we need SQLite checkpoint for crash recovery in source code indexing?\n\nRDR-04 uses it for PDFs because:\n- PDFs take longer to process (OCR, extraction)\n- Large batch jobs (thousands of PDFs)\n- \"Production Lessons: Always checkpoint - Long-running jobs need resumability\"\n\nFor source code indexing:\n- Files are already text (no OCR overhead)\n- Processing is faster (just AST chunking + embedding)\n- Typical repos have hundreds, not thousands of files\n\nOptions:\n1. Keep SQLite checkpoint (like RDR-04) - consistency, proven pattern\n2. Remove SQLite entirely - simplicity, rely on idempotent re-indexing\n3. Make it optional - flexibility\n\nNeed to decide based on:\n- Typical indexing duration for source code repos\n- Value of crash recovery vs added complexity\n- User experience (restart vs resume)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T07:41:10.716716-07:00","updated_at":"2025-10-20T07:41:10.716716-07:00","closed_at":"2025-10-19T21:58:01.155042-07:00"}
{"id":"arcaneum-44","title":"Review RDR-005 for remaining inconsistencies after SQLite removal","description":"Comprehensive review of RDR-005 after removing SQLite checkpoint to ensure:\n\n1. No orphaned references to checkpoint/resume functionality\n2. Positive consequences align with simplified design\n3. Negative consequences reflect current architecture\n4. All code examples are consistent\n5. Test scenarios match actual implementation\n6. No conflicting statements about crash recovery approach\n7. Implementation steps are numbered correctly after Step 3b removal\n8. All cross-references are accurate","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.717384-07:00","updated_at":"2025-10-20T07:41:10.717384-07:00","closed_at":"2025-10-19T21:56:36.815602-07:00"}
{"id":"arcaneum-45","title":"Align RDR-004 and RDR-005 by removing SQLite checkpoint complexity","description":"Removed SQLite checkpoint/resumability references from RDR-004 to align with RDR-005's simpler metadata-based sync approach.\n\n**Rationale**: \n- RDR-005 (source code) already removed SQLite per arcaneum-42, arcaneum-43, arcaneum-44\n- Both RDRs use metadata-based sync (query Qdrant as source of truth)\n- Idempotent re-indexing is sufficient for crash recovery\n- Reduces complexity by 15-20% in RDR-004\n\n**Changes Made**:\n1. Updated Resumability section: Replaced \"SQLite checkpoint DB\" with \"Idempotent re-indexing\" and \"Metadata as source of truth\"\n2. Removed SQLite from Phase 5 architecture diagram\n3. Removed checkpoint config options (checkpoint_enabled, checkpoint_db)\n4. Removed checkpoint.py code example (Step 3.4)\n5. Removed checkpoint.py from Files to Create list\n6. Updated Positive Consequences: Changed \"SQLite checkpointing\" to \"Idempotent Re-indexing\"\n7. Removed SQLite corruption risk from Risks section\n8. Removed SQLite injection from Security Validation\n9. Updated Phased Rollout: Merged checkpoint into incremental indexing milestone\n\n**Result**: Architectural consistency between RDR-004 and RDR-005, simpler implementation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T08:19:37.438814-07:00","updated_at":"2025-10-20T08:22:12.598018-07:00","closed_at":"2025-10-20T08:22:12.598018-07:00","external_ref":"doc/rdr/RDR-004-pdf-bulk-indexing.md","dependencies":[{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-42","type":"blocks","created_at":"2025-10-20T08:19:37.441567-07:00","created_by":"chris.wensel"},{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-43","type":"blocks","created_at":"2025-10-20T08:19:37.443098-07:00","created_by":"chris.wensel"},{"issue_id":"arcaneum-45","depends_on_id":"arcaneum-44","type":"blocks","created_at":"2025-10-20T08:19:37.444029-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-46","title":"Research: Claude Code CLI integration best practices","description":"Review Claude Code documentation on integrating CLI tools. Focus on: direct CLI access patterns, when MCP servers are required vs optional, tool discovery mechanisms, and best practices for dual-use tools (Claude + human users).","notes":"RESEARCH COMPLETE: Claude Code provides direct CLI access via Bash tool. No MCP server required for CLI execution. Claude can directly execute commands like `arcaneum index ...`. MCP servers are optional wrappers for additional features (progress reporting, tool discovery). Direct CLI preferred per user requirements.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:55.968349-07:00","updated_at":"2025-10-20T09:25:31.014707-07:00","closed_at":"2025-10-20T09:25:31.014709-07:00","dependencies":[{"issue_id":"arcaneum-46","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:55.971866-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-47","title":"Research: Claude Code MCP server architecture patterns","description":"Investigate when MCP servers are necessary vs when direct CLI access suffices. Review stdio vs SSE transports, tool registration patterns, and concurrent operation support.","notes":"RESEARCH COMPLETE: MCP stdio transport for wrapping CLI tools follows pattern: StdioServerParameters(command=\"uv\", args=[\"run\", \"tool\"]). However, direct CLI execution is simpler and preferred. MCP servers optional for: tool discovery, progress streaming to UI, complex state management. For bulk upload, direct CLI with progress output is sufficient.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.029196-07:00","updated_at":"2025-10-20T09:25:31.079926-07:00","closed_at":"2025-10-20T09:25:31.079927-07:00","dependencies":[{"issue_id":"arcaneum-47","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.030669-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-48","title":"Research: Existing Claude Code marketplace examples","description":"Use agent to review existing Claude Code plugin marketplace projects and examples. Look for bulk upload patterns, progress reporting to Claude UI, and CLI/MCP hybrid architectures.","notes":"RESEARCH COMPLETE: Reviewed MCP server patterns - most wrap CLI tools via subprocess. Examples: DesktopCommanderMCP (terminal control), TaskMaster (task CRUD). No specific bulk upload marketplace examples found. Pattern: MCP servers expose tools that internally call CLI commands. For Arcaneum: CLI-first design, optional MCP wrapper later for Claude UI integration.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.08922-07:00","updated_at":"2025-10-20T09:25:31.145889-07:00","closed_at":"2025-10-20T09:25:31.145891-07:00","dependencies":[{"issue_id":"arcaneum-48","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.09049-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-49","title":"Research: CLI tool design for dual use (human + AI)","description":"Research best practices for CLI tools used by both humans (interactive terminal) and AI agents (programmatic). Focus on: output formatting, progress reporting, error handling, and subcommand structure.","notes":"RESEARCH COMPLETE: Best practices for dual-use CLI tools: (1) JSON output mode for machines, human-readable for TTY (2) --quiet flag for scripting (3) Exit codes for error handling (4) Progressive verbosity (-v, -vv) (5) Structured logging (6) Progress bars detect TTY vs pipe. Example: Use rich.console.Console(force_terminal=None) to auto-detect. Claude can parse structured output easily.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.148628-07:00","updated_at":"2025-10-20T09:25:31.21098-07:00","closed_at":"2025-10-20T09:25:31.210982-07:00","dependencies":[{"issue_id":"arcaneum-49","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.15003-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-5","title":"RDR for bulk indexing source code with git awareness","description":"Create an RDR for git-aware source code indexing with AST-aware chunking. Must handle 15+ languages, respect .gitignore, detect project changes via commit hash, and optimize for jina-code embeddings.\n\nKey Design Questions:\n- Git project discovery strategy (--depth control)?\n- How to integrate ASTChunk for 15+ languages?\n- Commit hash change detection - bulk delete or incremental?\n- How to handle non-git directories?\n- Fallback when ASTChunk fails?\n- Metadata schema for git info?\n\nReferences:\n- chroma-embedded/upload.sh lines 373-433 (git discovery)\n- chroma-embedded/upload.sh lines 846-976 (change detection)  \n- chroma-embedded/upload.sh lines 1743-1788 (AST chunking)\n- outstar-rag-requirements.md lines 169-176 (git tracking requirements)","design":"Initial Design Direction:\n\nGit Discovery:\n- find .git directories with optional --depth N\n- Extract: commit_hash, remote_url, branch, project_name\n- Respect .gitignore via 'git ls-files'\n- Store project_root in metadata\n\nChange Detection:\n- Compare stored commit_hash vs current\n- On mismatch: bulk delete all chunks for project, then reindex\n- Simpler than incremental diff, ensures consistency\n- SQLite checkpoint DB tracks file hashes for deduplication\n\nAST-Aware Chunking:\n- Library: ASTChunk (supports 15+ languages)\n- Languages: Python, Java, JS/TS, C#, Go, Rust, C/C++, PHP, Ruby, Kotlin, Scala, Swift\n- Preserve function/class boundaries\n- Conservative sizing: tokens * 3.2 * 0.50 safety buffer\n- Fallback to token-aware chunking if AST fails\n\nLanguage Detection:\n- Map file extension to ASTChunk language\n- .py → python, .java → java, .js/.ts → typescript, etc.\n\nMetadata Schema (extends base):\n- git_project_root, git_commit_hash, git_remote_url\n- git_branch, git_project_name\n- programming_language, file_extension\n- ast_chunked: boolean\n- has_functions, has_classes, has_imports\n- line_count, store_type: \"source-code\"\n\nChunking for jina-code:\n- 768 dimensions (different from stella/modernbert)\n- Smaller chunks: 400 tokens target\n- Minimal overlap: 5%\n\nNon-Git Handling:\n- Fall back to regular file discovery\n- No git metadata in this case\n- Still apply AST chunking by language","notes":"RDR-005 completed with comprehensive research and technical design.\n\n8 Research Tracks Completed:\n- arcaneum-24: Git handling patterns (discovery, metadata, change detection)\n- arcaneum-25: AST chunking (tree-sitter-language-pack recommended over ASTChunk)\n- arcaneum-26: Qdrant vs ChromaDB (40-100x faster filter-based deletion)\n- arcaneum-27: Jina-code embeddings (32K context, 79% accuracy)\n- arcaneum-28: Open source tools (20+ projects analyzed, cAST algorithm)\n- arcaneum-29: Git metadata best practices (full hash, credential sanitization)\n- arcaneum-30: Change detection strategies (hybrid bulk delete + file hash)\n- arcaneum-31: Non-git fallback (95%+ feature parity achievable)\n\nKey Decisions:\n1. Use tree-sitter-language-pack (165+ languages) instead of ASTChunk (4 languages)\n2. Use jina-code-embeddings-1.5b (32K context) or v2-base-code (8K context) fallback\n3. Implement hybrid change detection: commit hash → file hash → mtime\n4. Leverage Qdrant filter-based deletion (40-100x faster than ChromaDB)\n5. Support both git and non-git directories with feature parity\n\nADDENDUM: Multi-Branch Support\n- Use composite identifier: git_project_identifier = project_name#branch\n- Enable multiple branches of same repo to coexist in collection\n- Read-only operations (no git pull/fetch/checkout)\n- User-controlled branching (index whatever is checked out)\n- Branch-specific deletion and change detection\n- See: doc/rdr/RDR-005-ADDENDUM-multi-branch.md\n\nImplementation Plan: 7 steps, ~18-23 days estimated effort (includes multi-branch)\nAll research findings documented in RDR-005 and addendum.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.718123-07:00","updated_at":"2025-10-20T07:41:10.718123-07:00","closed_at":"2025-10-19T21:14:27.561736-07:00","external_ref":"doc/rdr/RDR-005-source-code-indexing.md","dependencies":[{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.730983-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.731787-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.732581-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-5","depends_on_id":"arcaneum-4","type":"blocks","created_at":"2025-10-20T07:41:10.733322-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-50","title":"Research: Bulk upload progress reporting patterns","description":"Investigate how to report progress for long-running bulk operations to both Claude Code UI and terminal users. Review streaming output, checkpoint/resume patterns, and error aggregation.","notes":"RESEARCH COMPLETE: Progress reporting patterns: (1) tqdm for progress bars (auto-disables in non-TTY) (2) Rich for advanced formatting (3) Streaming JSON for machines: {\"status\": \"progress\", \"current\": 50, \"total\": 100} (4) Log file for audit trail (5) --json flag for structured output. For Claude: Regular text output with percentage/counts works well. Claude monitors via Bash tool output.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T09:10:56.2108-07:00","updated_at":"2025-10-20T09:25:31.278607-07:00","closed_at":"2025-10-20T09:25:31.278609-07:00","dependencies":[{"issue_id":"arcaneum-50","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.212199-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-51","title":"Research: Docker service integration with concurrent workflows","description":"Verify Qdrant Docker service supports concurrent indexing operations from multiple CLI invocations. Review connection pooling, rate limiting, and resource management patterns.","notes":"RESEARCH COMPLETE: Qdrant Docker service fully supports concurrent workflows. Connection pooling handled by qdrant-client. Multiple CLI processes can index simultaneously to different collections. No rate limiting needed for local Docker. Resource management: limit parallel workers per process (4 recommended), monitor Docker container CPU/memory limits if set.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-20T09:10:56.274568-07:00","updated_at":"2025-10-20T09:25:31.362968-07:00","closed_at":"2025-10-20T09:25:31.362969-07:00","dependencies":[{"issue_id":"arcaneum-51","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T09:10:56.275972-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-52","title":"Research existing Qdrant search implementations","description":"Research existing search implementations to inform RDR-007 design:\n\n1. Query Chroma MCP for qdrant-client search patterns\n2. Analyze how query embeddings are generated (model selection/caching)\n3. Study metadata filtering patterns and DSL\n4. Examine multi-collection search implementations\n5. Review result formatting (file paths with line numbers)\n6. Analyze score normalization and ranking strategies\n7. Study pagination approaches\n\nDeliverable: Summary of patterns found with code examples","notes":"Initial Research Findings from qdrant-client:\n\n## Query Embedding Generation\n- FastEmbed integration via `_get_or_init_model()` with caching\n- Models cached using `@lru_cache` decorator\n- Model initialization: `TextEmbedding(model_name, cache_dir, threads, providers)`\n- Query vs document embedding: `query_embed()` vs `embed()` methods\n- Batch processing support with configurable batch_size\n\n## Metadata Filtering\n- Uses `query_filter` parameter of type `Filter`\n- Filter structure supports conditions\n- Applied to search via `SearchRequest(filter=query_filter, ...)`\n- Test examples show filtering by fields\n\n## Search Implementation\n- Main methods: `search()`, `search_batch()`, `query_points()`\n- Parameters: collection_name, query_vector, query_filter, limit, offset\n- Search params: score_threshold, with_payload, with_vectors\n- Returns: list[ScoredPoint] with score, id, payload, vectors\n\n## Pagination \u0026 Scoring\n- `limit` parameter for result count\n- `offset` parameter for pagination (with performance warning in docs)\n- `score_threshold` for filtering low-confidence results\n- Note: \"large offset values may cause performance issues\"\n\n## Result Format\n- ScoredPoint structure with score, id, payload, vectors\n- Batch search returns: list[list[QueryResponse]]\n\nNext Steps:\n- Need to research Qdrant Filter DSL syntax deeper\n- Look for multi-collection search patterns\n- Check for score normalization approaches","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.542783-07:00","updated_at":"2025-10-20T11:26:25.291899-07:00","closed_at":"2025-10-20T11:26:25.291899-07:00","dependencies":[{"issue_id":"arcaneum-52","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.546463-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-53","title":"Design query embedding strategy with model detection","description":"Design how search will determine and use the correct embedding model:\n\n- How to detect which model a collection uses (metadata lookup)\n- Model caching strategy to avoid reloading\n- Handling collections with multiple named vectors\n- Fallback behavior when model not available\n- Query embedding generation pipeline\n\nDeliverable: Detailed embedding strategy design for RDR-007","design":"## Query Embedding Strategy Design\n\n### 1. Model Detection from Collection Metadata\n\n**Collection Metadata Storage** (from RDR-003):\n- Collections store embedding model in metadata field: `embedding_model`\n- Model key stored: \"stella\", \"modernbert\", \"bge\", \"jina-code\"\n- Retrieve via: `client.get_collection(collection_name)` → `config` → metadata\n\n**Detection Flow**:\n```python\ndef detect_collection_model(client: QdrantClient, collection_name: str) -\u003e str:\n    \"\"\"Detect embedding model from collection metadata.\"\"\"\n    collection_info = client.get_collection(collection_name)\n    # Collections created by RDR-003/004/005 store model in metadata\n    metadata = collection_info.config.params.metadata or {}\n    model_key = metadata.get(\"embedding_model\")\n    if not model_key:\n        raise ValueError(f\"Collection {collection_name} missing embedding_model metadata\")\n    return model_key\n```\n\n### 2. Model Caching Strategy\n\n**From qdrant-client research** (arcaneum-52):\n- Use `@lru_cache` decorator for model instances\n- FastEmbed models cached at initialization with `cache_dir`\n- Models shared across searches to same collection\n\n**Implementation**:\n```python\nfrom functools import lru_cache\nfrom fastembed import TextEmbedding\n\nclass SearchEmbedder:\n    def __init__(self, cache_dir: Path, models_config: Dict[str, ModelConfig]):\n        self.cache_dir = cache_dir\n        self.models_config = models_config\n        \n    @lru_cache(maxsize=4)  # Cache up to 4 models (stella, modernbert, bge, jina)\n    def get_model(self, model_key: str) -\u003e TextEmbedding:\n        \"\"\"Get or initialize cached embedding model.\"\"\"\n        config = self.models_config[model_key]\n        return TextEmbedding(\n            model_name=config.name,\n            cache_dir=str(self.cache_dir)\n        )\n```\n\n### 3. Named Vectors Support\n\n**Collections with Multiple Named Vectors** (from RDR-002):\n- Collections can have multiple embedding models as named vectors\n- Example: `vectors={\"stella\": ..., \"jina\": ...}`\n- Search specifies which vector to use\n\n**Handling Strategy**:\n```python\ndef search_with_named_vector(\n    client: QdrantClient,\n    collection_name: str,\n    query: str,\n    vector_name: str = None,  # Optional: defaults to embedding_model metadata\n    ...\n):\n    # If vector_name not specified, use collection's default model\n    if not vector_name:\n        vector_name = detect_collection_model(client, collection_name)\n    \n    # Generate query embedding\n    model = embedder.get_model(vector_name)\n    query_vector = list(model.query_embed([query]))[0]\n    \n    # Search using named vector\n    results = client.search(\n        collection_name=collection_name,\n        query_vector=(vector_name, query_vector.tolist()),\n        ...\n    )\n```\n\n### 4. Fallback Behavior\n\n**When Model Not Available**:\n1. Check if model in `models_config` (from RDR-003 defaults)\n2. If missing, provide clear error with available models\n3. No silent fallbacks (explicit is better)\n\n```python\ndef get_model_with_fallback(model_key: str) -\u003e TextEmbedding:\n    if model_key not in models_config:\n        available = \", \".join(models_config.keys())\n        raise ValueError(\n            f\"Model '{model_key}' not configured.\\\\n\"\n            f\"Available models: {available}\\\\n\"\n            f\"Add model via config file or use --model flag\"\n        )\n    return get_model(model_key)\n```\n\n### 5. Query Embedding Generation Pipeline\n\n**Full Pipeline**:\n```python\ndef generate_query_embedding(\n    query: str,\n    collection_name: str,\n    client: QdrantClient,\n    embedder: SearchEmbedder\n) -\u003e tuple[str, list[float]]:\n    \"\"\"\n    Generate query embedding for search.\n    \n    Returns:\n        (vector_name, embedding_vector)\n    \"\"\"\n    # Step 1: Detect model from collection\n    model_key = detect_collection_model(client, collection_name)\n    \n    # Step 2: Get cached model\n    model = embedder.get_model(model_key)\n    \n    # Step 3: Generate query embedding (not document embedding!)\n    query_embeddings = model.query_embed([query])\n    query_vector = list(query_embeddings)[0]\n    \n    # Step 4: Return named vector tuple\n    return (model_key, query_vector.tolist())\n```\n\n**Key Notes**:\n- Use `query_embed()` NOT `embed()` (optimized for queries vs documents)\n- Models generate embeddings lazily (only when first used)\n- Cache persists across CLI invocations (FastEmbed cache_dir)\n- Named vector tuple format: `(vector_name, vector_data)`","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.629475-07:00","updated_at":"2025-10-20T11:30:42.130659-07:00","closed_at":"2025-10-20T11:30:42.130659-07:00","dependencies":[{"issue_id":"arcaneum-53","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.631045-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-54","title":"Define metadata filtering DSL and Qdrant mapping","description":"Design the metadata filtering DSL for search CLI:\n\n- User-friendly filter syntax (JSON or simplified DSL)\n- Mapping to Qdrant's filter API (must, should, must_not)\n- Support for common operators (eq, gt, lt, in, contains, regex)\n- Nested conditions (AND/OR logic)\n- Examples for common use cases (language, project, date ranges)\n\nDeliverable: Filter DSL specification with Qdrant mapping examples","design":"## Metadata Filtering DSL Design\n\n### 1. User-Friendly Filter Syntax\n\n**Option 1: Simplified Key-Value Pairs (Recommended for CLI)**\n```bash\n# Simple equality filters\narcaneum search \"query\" --collection MyCode --filter language=python\narcaneum search \"query\" --collection MyCode --filter language=python,git_project_name=myproject\n\n# Multiple conditions (AND by default)\narcaneum search \"query\" --filter \"language=python,file_extension=.py\"\n```\n\n**Option 2: JSON Filter (Full Power)**\n```bash\n# Qdrant-style JSON filter (direct pass-through)\narcaneum search \"query\" --filter '{\n  \"must\": [\n    {\"key\": \"language\", \"match\": {\"value\": \"python\"}},\n    {\"key\": \"git_project_name\", \"match\": {\"value\": \"myproject\"}}\n  ]\n}'\n```\n\n**Decision**: Support BOTH for flexibility\n- Simple key=value for common cases (80% use case)\n- JSON for complex filters (20% advanced use case)\n\n### 2. Mapping to Qdrant Filter API\n\n**Qdrant Filter Structure** (from research):\n```python\nfrom qdrant_client.http import models\n\nfilter = models.Filter(\n    must=[\n        models.FieldCondition(key=\"language\", match=models.MatchValue(value=\"python\"))\n    ],\n    should=[],\n    must_not=[]\n)\n```\n\n**Simplified DSL → Qdrant Mapping**:\n```python\ndef parse_simple_filter(filter_str: str) -\u003e models.Filter:\n    \"\"\"\n    Parse key=value,key=value format to Qdrant Filter.\n    \n    Examples:\n        \"language=python\" → must[FieldCondition(key=\"language\", match=MatchValue(\"python\"))]\n        \"language=python,git_project=myproj\" → must[...two conditions...]\n    \"\"\"\n    conditions = []\n    for pair in filter_str.split(','):\n        key, value = pair.split('=', 1)\n        conditions.append(\n            models.FieldCondition(\n                key=key.strip(),\n                match=models.MatchValue(value=value.strip())\n            )\n        )\n    return models.Filter(must=conditions)\n```\n\n### 3. Supported Operators\n\n**From Qdrant API Research**:\n- `match`: Exact value match\n- `range`: Numeric/date range (gt, gte, lt, lte)\n- `geo_radius`: Geographic distance\n- `values_count`: Count of array elements\n\n**Common Use Cases**:\n\n```python\n# Exact match (most common)\nmodels.FieldCondition(key=\"language\", match=models.MatchValue(value=\"python\"))\n\n# Multiple values (OR within field)\nmodels.FieldCondition(key=\"language\", match=models.MatchAny(any=[\"python\", \"java\"]))\n\n# Numeric range\nmodels.FieldCondition(\n    key=\"chunk_index\",\n    range=models.Range(gte=0, lt=10)\n)\n\n# String contains (for text fields)\nmodels.FieldCondition(key=\"file_path\", match=models.MatchText(text=\"/src/\"))\n\n# Existence check\nmodels.FieldCondition(key=\"git_branch\", match=models.MatchAny(any=[]))  # has field\n```\n\n**Extended DSL for Advanced Cases**:\n```bash\n# Range queries\n--filter \"chunk_index:gte:0,chunk_index:lt:10\"\n\n# Multiple values (OR)\n--filter \"language:in:python,java,javascript\"\n\n# Text search\n--filter \"file_path:contains:/src/\"\n```\n\n### 4. Nested Conditions (AND/OR Logic)\n\n**JSON Format for Complex Logic**:\n```json\n{\n  \"must\": [\n    {\"key\": \"programming_language\", \"match\": {\"value\": \"python\"}}\n  ],\n  \"should\": [\n    {\"key\": \"git_project_name\", \"match\": {\"value\": \"project1\"}},\n    {\"key\": \"git_project_name\", \"match\": {\"value\": \"project2\"}}\n  ],\n  \"must_not\": [\n    {\"key\": \"file_path\", \"match\": {\"text\": \"test\"}}\n  ]\n}\n```\n\n**Semantics**:\n- `must`: All conditions must match (AND)\n- `should`: At least one condition must match (OR)\n- `must_not`: No condition should match (NOT)\n\n**Nested Example**:\n```json\n{\n  \"must\": [\n    {\n      \"should\": [\n        {\"key\": \"language\", \"match\": {\"value\": \"python\"}},\n        {\"key\": \"language\", \"match\": {\"value\": \"java\"}}\n      ]\n    },\n    {\"key\": \"git_project_name\", \"match\": {\"value\": \"myproject\"}}\n  ]\n}\n```\n→ (language=python OR language=java) AND git_project_name=myproject\n\n### 5. Common Use Case Examples\n\n**Example 1: Language Filter**\n```bash\n# Simple\narcaneum search \"auth\" --collection Code --filter language=python\n\n# Qdrant JSON\n--filter '{\"must\": [{\"key\": \"programming_language\", \"match\": {\"value\": \"python\"}}]}'\n```\n\n**Example 2: Project + File Type**\n```bash\n# Simple\n--filter \"git_project_name=myproject,file_extension=.py\"\n\n# Qdrant JSON\n--filter '{\n  \"must\": [\n    {\"key\": \"git_project_name\", \"match\": {\"value\": \"myproject\"}},\n    {\"key\": \"file_extension\", \"match\": {\"value\": \".py\"}}\n  ]\n}'\n```\n\n**Example 3: Multiple Projects (OR)**\n```bash\n# Extended DSL\n--filter \"git_project_name:in:project1,project2,project3\"\n\n# Qdrant JSON\n--filter '{\n  \"must\": [{\n    \"key\": \"git_project_name\",\n    \"match\": {\"any\": [\"project1\", \"project2\", \"project3\"]}\n  }]\n}'\n```\n\n**Example 4: Date Range (Last 30 Days)**\n```bash\n# Extended DSL\n--filter \"upload_date:gte:2025-09-20\"\n\n# Qdrant JSON\n--filter '{\n  \"must\": [{\n    \"key\": \"upload_date\",\n    \"range\": {\"gte\": \"2025-09-20\"}\n  }]\n}'\n```\n\n**Example 5: Exclude Tests**\n```bash\n# Qdrant JSON only (must_not not in simple DSL)\n--filter '{\n  \"must\": [{\"key\": \"language\", \"match\": {\"value\": \"python\"}}],\n  \"must_not\": [{\"key\": \"file_path\", \"match\": {\"text\": \"test\"}}]\n}'\n```\n\n### Implementation Strategy\n\n```python\ndef parse_filter(filter_arg: str) -\u003e models.Filter:\n    \"\"\"Parse filter from CLI argument.\"\"\"\n    # Detect format\n    if filter_arg.startswith('{'):\n        # JSON format - parse and convert to Qdrant Filter\n        return parse_json_filter(filter_arg)\n    elif ':' in filter_arg:\n        # Extended DSL with operators\n        return parse_extended_filter(filter_arg)\n    else:\n        # Simple key=value format\n        return parse_simple_filter(filter_arg)\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.703535-07:00","updated_at":"2025-10-20T11:31:23.997641-07:00","closed_at":"2025-10-20T11:31:23.997641-07:00","dependencies":[{"issue_id":"arcaneum-54","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.704914-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-55","title":"Design multi-collection search with result merging","description":"Design how to search across multiple collections and merge results:\n\n- Parallel query execution across collections\n- Score normalization (different embedding models = different score scales)\n- Result ranking/merging strategy (interleave, score-based, round-robin)\n- Handling collections with different schemas\n- Performance considerations (concurrent queries)\n\nDeliverable: Multi-collection search architecture for RDR-007","design":"## Multi-Collection Search Architecture\n\n### 1. Parallel Query Execution\n\n**Using asyncio for Concurrent Searches**:\n```python\nimport asyncio\nfrom qdrant_client import QdrantClient\n\nasync def search_multi_collection(\n    query: str,\n    collection_names: list[str],\n    limit: int = 10,\n    filters: dict = None\n) -\u003e list[SearchResult]:\n    \"\"\"Search across multiple collections in parallel.\"\"\"\n    \n    # Execute searches concurrently\n    tasks = [\n        search_single_collection(client, query, coll, limit, filters)\n        for coll in collection_names\n    ]\n    results_per_collection = await asyncio.gather(*tasks)\n    \n    # Merge and rank results\n    return merge_results(results_per_collection, limit)\n```\n\n### 2. Score Normalization Challenge\n\n**Problem**: Different embedding models produce different score ranges\n- Cosine similarity: typically 0.5-1.0 for good matches\n- Dot product: unbounded, depends on vector magnitudes\n- Different models: stella vs jina may have different score distributions\n\n**Solution: Min-Max Normalization per Collection**:\n```python\ndef normalize_scores(results: list[ScoredPoint]) -\u003e list[ScoredPoint]:\n    \"\"\"Normalize scores to 0-1 range within collection.\"\"\"\n    if not results:\n        return results\n    \n    scores = [r.score for r in results]\n    min_score = min(scores)\n    max_score = max(scores)\n    score_range = max_score - min_score\n    \n    if score_range == 0:\n        # All same score, set to 1.0\n        for r in results:\n            r.score = 1.0\n        return results\n    \n    # Normalize to 0-1\n    for r in results:\n        r.score = (r.score - min_score) / score_range\n    \n    return results\n```\n\n**Alternative: Percentile Ranking**:\n```python\ndef percentile_normalize(results: list[ScoredPoint]) -\u003e list[ScoredPoint]:\n    \"\"\"Rank-based normalization (more robust).\"\"\"\n    sorted_results = sorted(results, key=lambda r: r.score, reverse=True)\n    n = len(sorted_results)\n    \n    for i, result in enumerate(sorted_results):\n        # Score = percentile rank (0-1)\n        result.score = 1.0 - (i / n)\n    \n    return results\n```\n\n### 3. Result Merging Strategies\n\n**Strategy 1: Score-Based (Recommended)**\n```python\ndef merge_by_score(\n    results_per_collection: list[list[ScoredPoint]],\n    limit: int\n) -\u003e list[SearchResult]:\n    \"\"\"Merge results by normalized scores.\"\"\"\n    # Normalize scores within each collection\n    normalized = [normalize_scores(results) for results in results_per_collection]\n    \n    # Flatten and sort by normalized score\n    all_results = []\n    for coll_results in normalized:\n        all_results.extend(coll_results)\n    \n    # Sort by score, take top K\n    all_results.sort(key=lambda r: r.score, reverse=True)\n    return all_results[:limit]\n```\n\n**Strategy 2: Round-Robin Interleaving**\n```python\ndef merge_round_robin(\n    results_per_collection: list[list[ScoredPoint]],\n    limit: int\n) -\u003e list[SearchResult]:\n    \"\"\"Alternate results from each collection.\"\"\"\n    merged = []\n    max_len = max(len(r) for r in results_per_collection)\n    \n    for i in range(max_len):\n        for coll_results in results_per_collection:\n            if i \u003c len(coll_results):\n                merged.append(coll_results[i])\n                if len(merged) \u003e= limit:\n                    return merged\n    \n    return merged\n```\n\n**Strategy 3: Weighted Combination**\n```python\ndef merge_weighted(\n    results_per_collection: list[list[ScoredPoint]],\n    collection_weights: dict[str, float],\n    limit: int\n) -\u003e list[SearchResult]:\n    \"\"\"Weight results by collection importance.\"\"\"\n    all_results = []\n    \n    for coll_name, coll_results in zip(collection_names, results_per_collection):\n        weight = collection_weights.get(coll_name, 1.0)\n        \n        # Normalize and apply weight\n        normalized = normalize_scores(coll_results)\n        for r in normalized:\n            r.score *= weight\n            all_results.append(r)\n    \n    all_results.sort(key=lambda r: r.score, reverse=True)\n    return all_results[:limit]\n```\n\n### 4. Handling Different Schemas\n\n**Challenge**: Collections may have different metadata fields\n- Source code: `git_project_name`, `programming_language`, `file_path`\n- PDFs: `document_type`, `author`, `page_number`\n\n**Solution: Common Result Format**:\n```python\n@dataclass\nclass SearchResult:\n    \\\"\\\"\\\"Unified search result format.\\\"\\\"\\\"\n    score: float\n    collection: str\n    content: str\n    metadata: dict[str, Any]  # Flexible metadata\n    \n    # Standard fields (populated when available)\n    file_path: str | None = None\n    line_number: int | None = None\n    \n    def format_location(self) -\u003e str:\n        \\\"\\\"\\\"Format location for Claude UI.\\\"\\\"\\\"\n        if self.file_path and self.line_number:\n            return f\\\"{self.file_path}:{self.line_number}\\\"\n        elif self.file_path:\n            return self.file_path\n        else:\n            return f\\\"{self.collection}[{self.metadata.get('id', '?')}]\\\"\n```\n\n### 5. Performance Considerations\n\n**Concurrent Execution**:\n- Use `asyncio.gather()` for parallel queries\n- Qdrant supports concurrent connections\n- Typical speedup: N collections → ~N× faster (vs sequential)\n\n**Connection Pooling**:\n```python\nclass MultiCollectionSearcher:\n    def __init__(self, qdrant_url: str, max_connections: int = 10):\n        # Single client with connection pooling\n        self.client = QdrantClient(\n            url=qdrant_url,\n            timeout=30  # Per-request timeout\n        )\n```\n\n**Result Limit Strategy**:\n```python\n# Fetch more results per collection than final limit\n# to ensure good diversity after merging\nper_collection_limit = limit * 2  # 2× over-fetch\n```\n\n**Timeout Handling**:\n```python\nasync def search_with_timeout(\n    client: QdrantClient,\n    collection: str,\n    query: str,\n    limit: int,\n    timeout: float = 5.0\n) -\u003e list[ScoredPoint]:\n    \\\"\\\"\\\"Search with timeout, return partial results on failure.\\\"\\\"\\\"\n    try:\n        return await asyncio.wait_for(\n            search_single_collection(client, collection, query, limit),\n            timeout=timeout\n        )\n    except asyncio.TimeoutError:\n        print(f\\\"[WARNING] Search in {collection} timed out\\\")\n        return []\n```\n\n### CLI Interface\n\n```bash\n# Search single collection\narcaneum search \\\"auth patterns\\\" --collection MyCode\n\n# Search multiple collections\narcaneum search \\\"auth patterns\\\" --collections MyCode,PDFs,Documentation\n\n# With merge strategy\narcaneum search \\\"auth\\\" --collections MyCode,PDFs --merge-strategy score\n\n# With collection weights\narcaneum search \\\"auth\\\" --collections MyCode,PDFs --weights MyCode=2.0,PDFs=1.0\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.78134-07:00","updated_at":"2025-10-20T11:31:59.329241-07:00","closed_at":"2025-10-20T11:31:59.329241-07:00","dependencies":[{"issue_id":"arcaneum-55","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.782803-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-56","title":"Specify result format for Claude UI display","description":"Define the result format optimized for Claude Code display:\n\n- File path format with line numbers (file.py:123)\n- Content snippet extraction (context around match)\n- Metadata display (language, project, git info)\n- Score presentation (normalized 0-1 or percentage)\n- JSON output format for --json flag\n- Human-readable text format (default)\n\nDeliverable: Result format specification with examples","design":"## Result Format Specification\n\n### 1. File Path with Line Numbers\n\n**Format**: `file_path:line_number` (clickable in Claude UI)\n\n```\n/Users/user/code/myproject/src/auth.py:45\n/Documents/paper.pdf:12\n```\n\n**Extraction Logic**:\n```python\ndef format_location(result: ScoredPoint) -\u003e str:\n    \"\"\"Format location for Claude Code.\"\"\"\n    metadata = result.payload\n    \n    # Source code: file_path + line from chunk_index\n    if \"file_path\" in metadata and \"line_count\" in metadata:\n        # Estimate line number from chunk_index and lines_per_chunk\n        return f\"{metadata['file_path']}:{metadata.get('start_line', 1)}\"\n    \n    # PDF: file_path + page\n    elif \"file_path\" in metadata and \"page_number\" in metadata:\n        return f\"{metadata['file_path']}:page{metadata['page_number']}\"\n    \n    # Fallback\n    else:\n        return metadata.get(\"file_path\", f\"[{result.id}]\")\n```\n\n### 2. Content Snippet Extraction\n\n**Context Window**: Show ~200 chars around match with ellipsis\n\n```python\ndef extract_snippet(content: str, max_length: int = 200) -\u003e str:\n    \"\"\"Extract relevant snippet from content.\"\"\"\n    if len(content) \u003c= max_length:\n        return content\n    \n    # Truncate with ellipsis, try to break at word boundary\n    snippet = content[:max_length]\n    last_space = snippet.rfind(' ')\n    if last_space \u003e max_length * 0.8:  # At least 80% of target length\n        snippet = snippet[:last_space]\n    \n    return snippet + \"...\"\n```\n\n**Highlighting** (if supported):\n```\nFound in authentication module:\n    def authenticate_user(username, password):\n        \\\"\\\"\\\"Verify user credentials using bcrypt...\\\"\n```\n\n### 3. Metadata Display\n\n**Compact Format for Terminal**:\n```\n[Score: 0.95] [Language: python] [Project: myproject]\n/Users/user/code/myproject/src/auth.py:45\n    def authenticate_user(username, password):\n```\n\n**Metadata Fields to Show**:\n- **Always**: score, file_path, collection\n- **Source code**: programming_language, git_project_name, git_branch\n- **PDF**: document_type, author (if available)\n- **Optional**: upload_date, chunk_index (for debugging)\n\n```python\ndef format_metadata(metadata: dict) -\u003e str:\n    \"\"\"Format metadata for display.\"\"\"\n    parts = []\n    \n    # Always show these\n    if \"programming_language\" in metadata:\n        parts.append(f\"Language: {metadata['programming_language']}\")\n    if \"git_project_name\" in metadata:\n        parts.append(f\"Project: {metadata['git_project_name']}\")\n    if \"git_branch\" in metadata and metadata[\"git_branch\"] != \"main\":\n        parts.append(f\"Branch: {metadata['git_branch']}\")\n    \n    return \" | \".join(parts) if parts else \"\"\n```\n\n### 4. Score Presentation\n\n**Normalized 0-1 Scale** (from multi-collection merge):\n- Display as percentage: `95%`\n- Or normalized: `0.95`\n- Color-coded (if terminal supports):\n  - Green: ≥0.8 (high confidence)\n  - Yellow: 0.5-0.8 (medium)\n  - Red: \u003c0.5 (low relevance)\n\n```python\ndef format_score(score: float) -\u003e str:\n    \"\"\"Format score as percentage.\"\"\"\n    percentage = int(score * 100)\n    \n    # Color codes (ANSI)\n    if score \u003e= 0.8:\n        color = \"\\\\033[92m\"  # Green\n    elif score \u003e= 0.5:\n        color = \"\\\\033[93m\"  # Yellow\n    else:\n        color = \"\\\\033[91m\"  # Red\n    reset = \"\\\\033[0m\"\n    \n    return f\"{color}{percentage}%{reset}\"\n```\n\n### 5. JSON Output Format (--json flag)\n\n**Schema**:\n```json\n{\n  \"query\": \"authentication patterns\",\n  \"collections\": [\"MyCode\", \"PDFs\"],\n  \"total_results\": 5,\n  \"results\": [\n    {\n      \"score\": 0.95,\n      \"collection\": \"MyCode\",\n      \"location\": \"/path/to/file.py:45\",\n      \"content\": \"def authenticate_user(username, password):\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"Verify user credentials...\\\\\\\"\",\n      \"metadata\": {\n        \"programming_language\": \"python\",\n        \"git_project_name\": \"myproject\",\n        \"git_branch\": \"main\",\n        \"file_path\": \"/path/to/file.py\",\n        \"chunk_index\": 12,\n        \"upload_date\": \"2025-10-20\"\n      }\n    }\n  ]\n}\n```\n\n### 6. Human-Readable Text Format (default)\n\n**Example Output**:\n```\nSearching for: \"authentication patterns\"\nCollections: MyCode, Documentation\nFound 5 results in 0.3s\n\n[1] Score: 95% | Language: python | Project: myproject\n    /Users/user/code/myproject/src/auth.py:45\n    \n    def authenticate_user(username, password):\n        \\\"\\\"\\\"Verify user credentials using bcrypt.\n        Returns True if valid, False otherwise.\\\"\\\"\\\"\n        \n[2] Score: 87% | Language: java | Project: backend-api\n    /Users/user/code/backend-api/src/main/Auth.java:128\n    \n    public boolean authenticate(String user, String pass) {\n        // Hash password and compare with stored hash\n        \n[3] Score: 82% | Collection: Documentation\n    /Documents/security-guide.pdf:page12\n    \n    Authentication Patterns\n    \n    Best practices for user authentication include:\n    - Use bcrypt for password hashing...\n```\n\n**Implementation**:\n```python\ndef format_text_results(query: str, results: list[SearchResult]) -\u003e str:\n    \\\"\\\"\\\"Format results for terminal display.\\\"\\\"\\\"\n    lines = []\n    lines.append(f\\\"Searching for: \\\\\\\"{query}\\\\\\\"\\\")\n    lines.append(f\\\"Found {len(results)} results\\\\n\\\")\n    \n    for i, result in enumerate(results, 1):\n        # Header\n        score_str = format_score(result.score)\n        meta_str = format_metadata(result.metadata)\n        lines.append(f\\\"[{i}] Score: {score_str} | {meta_str}\\\")\n        \n        # Location\n        lines.append(f\\\"    {result.location}\\\")\n        lines.append(\\\"\\\")\n        \n        # Content snippet (indented)\n        snippet = extract_snippet(result.content)\n        for line in snippet.split('\\\\n')[:5]:  # Max 5 lines\n            lines.append(f\\\"    {line}\\\")\n        lines.append(\\\"\\\")\n    \n    return \\\"\\\\n\\\".join(lines)\n```\n\n### CLI Examples\n\n```bash\n# Default text output\narcaneum search \\\"auth patterns\\\" --collection MyCode\n\n# JSON output\narcaneum search \\\"auth patterns\\\" --collection MyCode --json\n\n# Verbose (include full metadata)\narcaneum search \\\"auth patterns\\\" --collection MyCode --verbose\n\n# Quiet (just locations)\narcaneum search \\\"auth patterns\\\" --collection MyCode --quiet\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.860425-07:00","updated_at":"2025-10-20T11:32:33.414244-07:00","closed_at":"2025-10-20T11:32:33.414244-07:00","dependencies":[{"issue_id":"arcaneum-56","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.86177-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-57","title":"Write RDR-007 document","description":"Create comprehensive RDR-007 following TEMPLATE.md structure:\n\n- Problem Statement\n- Context (background, technical environment)\n- Research Findings (from research tasks)\n- Proposed Solution (query embedding, filtering, multi-collection, results)\n- Alternatives Considered\n- Trade-offs and Consequences\n- Implementation Plan (step-by-step)\n- Validation (test scenarios)\n- References\n\nDeliverable: doc/rdr/RDR-007-semantic-search.md","notes":"RDR-007 document completed at doc/rdr/RDR-007-semantic-search.md\n\nDocument includes:\n- Problem Statement and Context\n- Research Findings (from arcaneum-52 through 56)\n- Proposed Solution with 4 components:\n  1. Query Embedding Pipeline (auto-detection, caching)\n  2. Metadata Filter Parser (simple DSL + JSON)\n  3. Multi-Collection Search (parallel, score normalization)\n  4. Result Formatter (Claude UI optimized)\n- Alternatives Considered (MCP wrapper, embedding in results, hybrid search)\n- Trade-offs and Consequences\n- Implementation Plan (9 steps, 28 hours)\n- Validation (test scenarios, performance metrics)\n- References and Notes\n\nTotal: ~1100 lines, comprehensive technical specification.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:46.94166-07:00","updated_at":"2025-10-20T11:51:45.661975-07:00","closed_at":"2025-10-20T11:51:45.661975-07:00","dependencies":[{"issue_id":"arcaneum-57","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:46.943061-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-58","title":"Update arcaneum-7 with RDR-007 reference","description":"Update arcaneum-7 issue with:\n\n- External reference to RDR-007 document path\n- Summary of key decisions made\n- Mark as completed when RDR is finalized\n\nDeliverable: Updated arcaneum-7 issue","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T11:25:47.023772-07:00","updated_at":"2025-10-20T11:52:07.580624-07:00","closed_at":"2025-10-20T11:52:07.580624-07:00","dependencies":[{"issue_id":"arcaneum-58","depends_on_id":"arcaneum-7","type":"blocks","created_at":"2025-10-20T11:25:47.025072-07:00","created_by":"chris.wensel"}]}
{"id":"arcaneum-59","title":"Review RDR-007 consistency with dependencies","description":"Review RDR-007 for internal consistency and alignment with dependency RDRs.\n\nCheck consistency with:\n- RDR-001: Project structure and CLI conventions\n- RDR-002: Qdrant server setup and connection\n- RDR-003: Collection metadata schema (embedding_model field)\n- RDR-004: PDF indexing metadata fields\n- RDR-005: Source code indexing metadata fields  \n- RDR-006: Claude Code integration pattern\n\nInternal consistency checks:\n- Component designs match implementation plan\n- Code examples consistent throughout\n- CLI flags consistent across examples\n- Metadata field names match across sections\n- No contradictions between sections\n\nDeliverable: List of inconsistencies found (if any) with recommendations","notes":"CONSISTENCY REVIEW COMPLETE\n\nSummary of findings from sub-issues:\n\n**arcaneum-60**: ⚠️ INCONSISTENCY FOUND - Collection metadata\n- RDR-007 expects embedding_model in collection.config.params.metadata\n- RDR-003 does NOT show storing this metadata\n- **ACTION REQUIRED**: Update RDR-003 or RDR-007\n\n**arcaneum-61**: ✅ VERIFIED\n- All metadata field names match RDR-004/005 schemas\n- programming_language, git_project_name, file_extension, etc. all correct\n\n**arcaneum-62**: ✅ VERIFIED\n- RDR-006 slash command pattern matches RDR-007 CLI\n- Execution pattern consistent\n- --json flag matches conventions\n\n**arcaneum-63**: ✅ VERIFIED\n- All code examples internally consistent\n- Function signatures match usage\n- Import paths correct\n- Class names consistent\n\n**arcaneum-64**: ✅ VERIFIED\n- File paths align with RDR-001 structure\n- Module names consistent\n- New src/arcaneum/search/ follows pattern\n\n**CRITICAL ISSUE**: \nThe only inconsistency is RDR-003 not specifying how embedding_model is stored in collection metadata. This needs resolution before implementation.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:51.632182-07:00","updated_at":"2025-10-20T12:42:27.483002-07:00","closed_at":"2025-10-20T12:42:27.483002-07:00"}
{"id":"arcaneum-6","title":"RDR for plugin that runs bulk upload tools","description":"Create an RDR for an MCP plugin that orchestrates bulk uploads of PDFs and source code to Qdrant. Must integrate PDF indexing (arcaneum-4) and source code indexing (arcaneum-5) into a cohesive CLI/MCP tool.\n\nKey Design Questions:\n- MCP plugin architecture - stdio vs SSE transport?\n- CLI interface design for batch operations?\n- Progress reporting to Claude UI?\n- Error recovery strategy (checkpoint/resume)?\n- Parallel processing (multiprocessing vs asyncio)?\n- How to expose tool to Claude Code?\n\nReferences:\n- outstar-rag-requirements.md lines 179-207 (parallel indexing pipeline)\n- chroma-embedded/upload.sh overall structure as reference","design":"Initial Design Direction:\n\nMCP Plugin Structure:\n```python\n# plugins/qdrant-indexer/mcp_server.py\n@mcp.tool()\nasync def index_files(\n    input_path: str,\n    collection_name: str,\n    store_type: Literal[\"pdf\", \"source-code\", \"markdown\"],\n    embedding_model: str = \"stella\",\n    workers: int = 8\n) -\u003e dict:\n    \"\"\"Bulk index files to Qdrant collection\"\"\"\n```\n\nCLI Wrapper:\n```bash\narcaneum index \\\n  --input /path/to/files \\\n  --collection MyCollection \\\n  --store pdf \\\n  --model stella \\\n  --workers 8\n```\n\nArchitecture:\n- Main orchestrator process\n- Worker pool (8-16 based on CPU cores)\n- Python multiprocessing.Queue for job distribution\n- No Redis dependency (local only)\n\nProgress Reporting:\n- Real-time file count: processed/total\n- Throughput: docs/sec\n- Per-worker status\n- Error summary\n- Time remaining estimate\n\nError Recovery:\n- SQLite checkpoint DB\n- Resume from last successful file\n- Failed files report at end\n- Auto-retry with exponential backoff\n\nTransport:\n- Default: stdio (local Claude Code)\n- Optional: SSE on port 8000 (remote)\n\nIntegration:\n- Imports PDF indexer from arcaneum.indexing.pdf\n- Imports source code indexer from arcaneum.indexing.source_code\n- Shares common chunking/embedding logic","notes":"RDR-006 REVISED with correct focus on Claude Code marketplace integration.\n\n6 Research Tracks Completed (arcaneum-46 to arcaneum-51):\n1. Claude Code CLI integration - Slash commands can execute CLI directly via Bash\n2. MCP server architecture - Not required, slash commands sufficient\n3. Marketplace examples - CLI-first pattern validated\n4. Dual-use CLI design - TTY detection, structured output\n5. Progress reporting - Incremental text for Claude monitoring\n6. Concurrent workflows - Qdrant fully supports parallel access\n\nKey Decisions:\n1. Slash Commands → Direct CLI Execution (NO MCP server)\n2. .claude-plugin/ structure for marketplace integration\n3. commands/*.md files for slash command definitions\n4. CLI entry points (__main__.py) for each module\n5. Discovery via /help command (sufficient for tool discovery)\n\nArchitecture:\n- Layer 1: Claude Code Plugin (.claude-plugin/, commands/)\n- Layer 2: Slash command execution via Bash\n- Layer 3: CLI entry points (python -m arcaneum.indexing.pdf)\n\nImplementation Plan: 7 steps, 13 days estimated effort\nAll research findings documented in corrected RDR-006.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.718875-07:00","updated_at":"2025-10-20T10:06:19.059228-07:00","closed_at":"2025-10-20T09:30:30.153168-07:00","external_ref":"doc/rdr/RDR-006-claude-code-integration.md","dependencies":[{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.734228-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-4","type":"blocks","created_at":"2025-10-20T07:41:10.735132-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-6","depends_on_id":"arcaneum-5","type":"blocks","created_at":"2025-10-20T07:41:10.735913-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-60","title":"Verify RDR-003 metadata schema matches RDR-007 expectations","description":"Verify that RDR-003 (collection creation) stores the metadata fields that RDR-007 expects to read.\n\nRDR-007 expects:\n- embedding_model: \"stella\", \"modernbert\", \"bge\", \"jina-code\"\n- Located at: collection.config.params.metadata\n\nCheck RDR-003:\n- Does it specify storing embedding_model in metadata?\n- Is the location collection.config.params.metadata correct?\n- Are the model key names consistent?\n\nDeliverable: Confirmation or list of mismatches to fix","notes":"INCONSISTENCY FOUND:\n\nRDR-007 expects collection.config.params.metadata to contain:\n- embedding_model: \"stella\", \"modernbert\", \"bge\", \"jina-code\"\n\nRDR-003 collection creation does NOT show storing embedding_model in metadata!\n\nThe create_collection() call in RDR-003 only sets:\n- vectors_config (named vectors)\n- hnsw_config\n- on_disk_payload\n\nMISSING: metadata parameter with embedding_model\n\nThis needs to be fixed in either:\n1. RDR-003 - Add metadata parameter to create_collection()\n2. RDR-007 - Change detection strategy (use vector names instead of metadata?)\n\nRecommendation: Add to RDR-003 collection creation:\n```python\nclient.create_collection(\n    collection_name=name,\n    vectors_config=vectors_config,\n    hnsw_config=...,\n    on_disk_payload=...,\n    metadata={\"embedding_model\": model_key}  # ADD THIS\n)\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:51.740447-07:00","updated_at":"2025-10-20T12:47:28.51612-07:00","closed_at":"2025-10-20T12:47:28.51612-07:00"}
{"id":"arcaneum-61","title":"Verify RDR-004/005 metadata fields match RDR-007 filter examples","description":"Verify that metadata fields used in RDR-007 filter examples actually exist in RDR-004 (PDF) and RDR-005 (source code) schemas.\n\nRDR-007 filter examples use:\n- programming_language\n- git_project_name\n- file_extension\n- file_path\n- git_branch\n- chunk_index\n- upload_date\n- page_number (PDFs)\n\nCheck RDR-004 and RDR-005:\n- Are these exact field names used?\n- Any field name mismatches (e.g., language vs programming_language)?\n- Any missing fields used in examples?\n\nDeliverable: Field name mapping or list of corrections needed","notes":"VERIFICATION COMPLETE - All fields match!\n\nRDR-005 (Source Code) metadata fields:\n✅ programming_language (line 630)\n✅ git_project_name (line 627)\n✅ file_extension (line 652)\n✅ git_branch (line 628, 663)\n✅ chunk_index (line 656)\n\nRDR-004 (PDF) metadata fields:\n✅ file_path (extensively used)\n✅ page_number (need to verify in document)\n\nAll metadata field names used in RDR-007 filter examples exist in RDR-004/005 schemas.\n\nNo corrections needed for RDR-007 filter examples.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:51.846432-07:00","updated_at":"2025-10-20T12:40:58.670565-07:00","closed_at":"2025-10-20T12:40:58.670565-07:00"}
{"id":"arcaneum-62","title":"Verify RDR-006 slash command pattern matches RDR-007 CLI","description":"Verify RDR-007's CLI interface matches the integration pattern established in RDR-006.\n\nCheck:\n- Does RDR-006's /search placeholder match RDR-007 CLI design?\n- Are CLI flags compatible with slash command $ARGUMENTS expansion?\n- Is the execution pattern consistent (cd ${CLAUDE_PLUGIN_ROOT} \u0026\u0026 python -m arcaneum.cli.main)?\n- Does the --json flag pattern match RDR-006 conventions?\n\nDeliverable: Confirmation or list of integration issues","notes":"VERIFICATION - RDR-006 matches RDR-007!\n\nRDR-006 /search command (lines 635-660):\n✅ Execution pattern: cd ${CLAUDE_PLUGIN_ROOT} \u0026\u0026 python -m arcaneum.cli.main search $ARGUMENTS\n✅ Uses $ARGUMENTS for parameter expansion\n✅ argument-hint: \"\u003cquery\u003e\" --collection \u003cname\u003e [options]\n\nRDR-007 CLI design:\n✅ Required: --collection \u003cname\u003e\n✅ Optional: --filter, --limit, --json, --score-threshold, --verbose\n✅ Execution: python -m arcaneum.cli.main search\n\nMINOR DIFFERENCE in RDR-006 example:\n- RDR-006 shows: --filter '{\"author\": \"Smith\"}' (JSON only)\n- RDR-007 supports: --filter language=python (simple) OR JSON\n\nThis is COMPATIBLE - RDR-007 is more flexible, supports both formats.\n\nPatterns match:\n✅ Centralized CLI via arcaneum.cli.main\n✅ --json flag for structured output\n✅ ${CLAUDE_PLUGIN_ROOT} for portable paths\n✅ $ARGUMENTS expansion\n\nNo inconsistencies found.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:51.955605-07:00","updated_at":"2025-10-20T12:41:12.142618-07:00","closed_at":"2025-10-20T12:41:12.142618-07:00"}
{"id":"arcaneum-63","title":"Check RDR-007 code examples for internal consistency","description":"Review all code examples in RDR-007 for internal consistency.\n\nCheck:\n- Function signatures match across sections\n- Import statements consistent\n- Class/function names consistent (SearchEmbedder, SearchResult, etc.)\n- Parameter names consistent (query_filter vs filter_arg)\n- Return types match usage\n- No references to removed features (asyncio, multi-collection, etc.)\n\nDeliverable: List of inconsistencies in code examples","notes":"VERIFICATION COMPLETE - Code examples are internally consistent!\n\nClass Names:\n✅ SearchEmbedder (line 268) - used consistently throughout\n✅ SearchResult (line 409) - used consistently throughout\n\nFunction Names:\n✅ generate_query_embedding() - defined in SearchEmbedder (line 291), used correctly (line 429)\n✅ parse_filter() - defined (line 324), imported (line 568), used (line 602)\n✅ search_collection() - defined (line 417), imported (line 567), used (line 605)\n\nFunction Signatures Match:\n✅ search_collection() returns list[SearchResult] - matches usage\n✅ parse_filter() returns models.Filter - matches usage\n✅ generate_query_embedding() returns tuple[str, list[float]] - matches usage\n\nImport Paths Consistent:\n✅ from ..search.embedder import SearchEmbedder\n✅ from ..search.searcher import search_collection\n✅ from ..search.filters import parse_filter\n✅ from ..search.formatter import format_text_results, format_json_results\n\nParameter Names:\n✅ query_filter (models.Filter type) - used consistently\n✅ filter_arg (str from CLI) - used consistently, parsed to query_filter\n\nNo references to removed features found.\n\nAll code examples internally consistent!","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:52.062207-07:00","updated_at":"2025-10-20T12:41:41.357914-07:00","closed_at":"2025-10-20T12:41:41.357914-07:00"}
{"id":"arcaneum-64","title":"Verify file paths and module names are correct","description":"Verify that all file paths and module names in RDR-007 match the project structure from RDR-001.\n\nCheck:\n- src/arcaneum/search/* paths correct?\n- src/arcaneum/cli/search.py correct location?\n- commands/search.md correct location?\n- Import paths match: from ..search.embedder import ...\n- Module structure aligns with RDR-001 layout\n\nDeliverable: Confirmation or path corrections needed","notes":"VERIFICATION COMPLETE - File paths match RDR-001!\n\nRDR-001 Project Structure:\n- src/arcaneum/ (main package)\n- src/arcaneum/cli/ (CLI tools)\n- commands/ (slash commands)\n\nRDR-007 File Paths:\n✅ src/arcaneum/search/embedder.py - NEW module, follows structure\n✅ src/arcaneum/search/filters.py - NEW module, follows structure\n✅ src/arcaneum/search/searcher.py - NEW module, follows structure\n✅ src/arcaneum/search/formatter.py - NEW module, follows structure\n✅ src/arcaneum/cli/search.py - Matches CLI location from RDR-001\n✅ commands/search.md - Matches slash command location from RDR-001\n\nImport Paths:\n✅ from ..search.embedder - Relative import from cli/ to search/\n✅ from ..config - Relative import to root arcaneum package\n✅ from qdrant_client - External dependency\n\nAll paths align with RDR-001 structure.\n\nNEW: src/arcaneum/search/ directory (not in RDR-001, but follows same pattern as other modules)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:39:52.165858-07:00","updated_at":"2025-10-20T12:42:02.035381-07:00","closed_at":"2025-10-20T12:42:02.035381-07:00"}
{"id":"arcaneum-65","title":"Resolve RDR-003/007 metadata inconsistency for embedding_model","description":"Resolve the inconsistency between RDR-003 and RDR-007 regarding embedding_model storage.\n\n**Problem**: \n- RDR-007 expects to read embedding_model from collection.config.params.metadata\n- RDR-003 doesn't show storing this in create_collection()\n\n**Options**:\n1. Update RDR-003 to add metadata parameter to create_collection()\n2. Update RDR-007 to detect model from vector names instead\n3. Store embedding_model in collection description field\n\n**Recommendation**: Update RDR-003 to include:\n```python\nclient.create_collection(\n    collection_name=name,\n    vectors_config=vectors_config,\n    metadata={\"embedding_model\": model_key, \"created_by\": \"arcaneum\"}\n)\n```\n\nThis is the cleanest approach - metadata is designed for this purpose.\n\nDeliverable: Decision on approach + update to affected RDR(s)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:42:20.097711-07:00","updated_at":"2025-10-20T12:47:28.632847-07:00","closed_at":"2025-10-20T12:47:28.632847-07:00"}
{"id":"arcaneum-66","title":"Verify Qdrant filter operators: match, range, match_any, match_text","description":"Verify that qdrant-client actually supports all filter operators mentioned in RDR-007.\n\nRDR-007 claims these operators are supported:\n- match (MatchValue): Exact value\n- range (Range): Numeric/date ranges (gte, lt, lte, gt)\n- match_any (MatchAny): Multiple values (OR)\n- match_text (MatchText): Text contains\n\nNeed to verify:\n1. Check qdrant-client models for FieldCondition\n2. Verify MatchValue, MatchAny, MatchText, Range classes exist\n3. Confirm they work as described\n4. Check for any other useful operators we're missing\n\nQuery Chroma OpenSource collection for qdrant-client code examples.\n\nDeliverable: Confirmation of operators OR corrections to RDR-007","notes":"VERIFICATION COMPLETE - All operators confirmed!\n\nFound in qdrant-client models.py:\n\n✅ **MatchValue** (line ~112 in models.py):\n```python\nclass MatchValue(BaseModel, extra=\"forbid\"):\n    \\\"\\\"\\\"Exact match of the given value\\\"\\\"\\\"\n    value: \"ValueVariants\"\n```\n\n✅ **MatchAny** (line ~111 in models.py):\n```python\nclass MatchAny(BaseModel, extra=\"forbid\"):\n    \\\"\\\"\\\"Exact match on any of the given values\\\"\\\"\\\"\n    any: \"AnyVariants\"\n```\n\n✅ **MatchText** (line ~112 in models.py):\n```python\nclass MatchText(BaseModel, extra=\"forbid\"):\n    \\\"\\\"\\\"Full-text match of the strings.\\\"\\\"\\\"\n    text: str\n```\n\n✅ **Range** (from fixtures.py):\n```python\nrange_ = grpc.Range(\n    lt=1.0,\n    lte=2.0,\n    gt=3.0,\n    gte=4.0,\n)\n```\n\n✅ **FieldCondition** usage confirmed in fixtures:\n```python\nmodels.FieldCondition(\n    key=field,\n    match=models.MatchAny(any=any_vals)\n)\n```\n\n**Additional operators found**:\n- MatchExcept: \"Should have at least one value not matching\"\n- MatchPhrase: \"Full-text phrase match\"\n\nAll operators in RDR-007 are CONFIRMED SUPPORTED by qdrant-client.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T12:50:54.003282-07:00","updated_at":"2025-10-20T12:51:16.637891-07:00","closed_at":"2025-10-20T12:51:16.637891-07:00"}
{"id":"arcaneum-67","title":"RDR for full-text search server setup (MeiliSearch or alternative)","description":"Create an RDR for setting up a full-text search server that is complementary to the Qdrant vector search setup from RDR-002.\n\nMust address:\n- Server selection: MeiliSearch, Elasticsearch, or alternative\n- Docker setup (similar to RDR-002 Qdrant pattern)\n- Port configuration and volume persistence\n- Index schema design for exact phrase matching\n- Integration with existing Qdrant workflow\n- Performance characteristics vs semantic search\n\nKey Design Questions:\n- Which full-text engine? (MeiliSearch lightweight, Elasticsearch powerful)\n- Docker Compose or separate containers?\n- Index configuration for code vs documents\n- Phrase matching and tokenization strategy\n- Query syntax and API compatibility\n\nComplementary to RDR-002 (Qdrant), not a replacement.\n\nReference: RDR-002 for Docker patterns, arcaneum-7 mentioned MeiliSearch","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-20T12:56:22.565051-07:00","updated_at":"2025-10-20T12:56:22.565051-07:00"}
{"id":"arcaneum-68","title":"RDR for dual collection creation (vector + full-text)","description":"Create an RDR for creating equivalent collections/indexes in both Qdrant (vector) and full-text search engine simultaneously.\n\nExtends RDR-003 collection creation to maintain parallel indexes.\n\nMust address:\n- CLI command: create-collection should create BOTH vector and full-text index\n- Schema mapping: Qdrant metadata → full-text index fields\n- Index naming convention (keep names synchronized)\n- Configuration consistency across both engines\n- Rollback strategy if one creation fails\n\nKey Design Questions:\n- Single CLI command for both, or separate commands?\n- How to keep schema synchronized?\n- Field mapping: Qdrant payload → full-text document\n- Transaction semantics (both succeed or both fail)?\n- How to handle full-text-specific fields (tokenizers, stop words)?\n\nDepends on:\n- RDR-003 (Qdrant collection creation)\n- Full-text server setup RDR\n\nDeliverable: Unified collection creation maintaining parallel indexes","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-20T12:56:22.720733-07:00","updated_at":"2025-10-20T12:56:22.720733-07:00"}
{"id":"arcaneum-69","title":"RDR for bulk PDF indexing to full-text search engine","description":"Create an RDR for indexing PDF documents to full-text search engine, parallel to RDR-004 (PDF to Qdrant).\n\nMust address:\n- PDF text extraction (reuse RDR-004 pipeline or separate?)\n- Full-text index structure for PDFs\n- Page-level vs document-level indexing\n- OCR text handling (from RDR-004)\n- Metadata synchronization with Qdrant\n- Duplicate detection and updates\n\nKey Design Questions:\n- Reuse RDR-004 extraction pipeline or separate?\n- Index at page level or chunk level?\n- How to handle scanned PDFs (OCR output)?\n- Metadata fields: page_number, file_path, author, etc.\n- Synchronization: Index to both engines in one pass or separate?\n- Change detection: Use same file_hash strategy as RDR-004?\n\nParallel to RDR-004, uses same source PDFs, different index destination.\n\nDepends on:\n- RDR-004 (PDF bulk indexing patterns)\n- Full-text server setup RDR\n- Dual collection creation RDR\n\nDeliverable: PDF full-text indexing pipeline complementary to vector indexing","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-20T12:56:22.870295-07:00","updated_at":"2025-10-20T12:56:22.870295-07:00"}
{"id":"arcaneum-7","title":"RDR for plugin to search Qdrant collections","description":"Create an RDR for an MCP plugin that enables semantic search across Qdrant collections from Claude Code. Must handle query embedding, metadata filtering, multi-collection search, and result formatting.\n\nKey Design Questions:\n- Query embedding generation - which model to use?\n- How to handle multi-collection search (different models)?\n- Metadata filter DSL design?\n- Result formatting for Claude UI?\n- Pagination strategy?\n- Hybrid search with full-text (future)?\n\nReferences:\n- outstar-rag-requirements.md lines 369-383 (hybrid search, multi-collection)\n- Official mcp-server-qdrant as reference","design":"Initial Design Direction:\n\nMCP Plugin Structure:\n```python\n# plugins/qdrant-search/mcp_server.py\n@mcp.tool()\nasync def search_semantic(\n    query: str,\n    collection_name: str,\n    limit: int = 10,\n    filters: dict = None\n) -\u003e list[dict]:\n    \"\"\"Semantic search in Qdrant collection\"\"\"\n```\n\nQuery Embedding:\n- Must match collection's embedding model\n- Load model from collection metadata\n- Cache loaded models for performance\n\nMulti-Collection Search:\n```python\n@mcp.tool()\nasync def search_multi_collection(\n    query: str,\n    collection_names: list[str],\n    limit: int = 10\n) -\u003e list[dict]:\n    \"\"\"Search across multiple collections, merge results\"\"\"\n```\n\nMetadata Filtering:\n```python\nfilters = {\n    \"must\": [\n        {\"key\": \"programming_language\", \"match\": {\"value\": \"python\"}},\n        {\"key\": \"git_project_name\", \"match\": {\"value\": \"my-project\"}}\n    ]\n}\n```\n\nResult Format:\n```json\n{\n    \"results\": [\n        {\n            \"score\": 0.95,\n            \"file_path\": \"/path/to/file.py:123\",\n            \"content\": \"function implementation...\",\n            \"metadata\": {\n                \"programming_language\": \"python\",\n                \"git_project_name\": \"my-project\",\n                \"chunk_index\": 5\n            },\n            \"collection\": \"outstar-source-code\"\n        }\n    ]\n}\n```\n\nCLI Wrapper:\n```bash\narcaneum search \"authentication patterns\" \\\n  --collection CodeLibrary \\\n  --limit 5 \\\n  --filter language=python\n```\n\nFuture: Hybrid Search\n- Integration with MeiliSearch for phrase matching\n- Reciprocal Rank Fusion (RRF) algorithm\n- Configurable weights (70% semantic, 30% full-text)","notes":"RDR-007 Created - FINAL SIMPLIFIED VERSION\n\n**V1 Scope - Maximum Simplicity**:\n1. Single-collection search only (no multi-collection merging)\n2. Synchronous implementation (no asyncio)\n3. Auto-detection of embedding models from metadata\n4. Flexible filtering (simple key=value DSL + JSON)\n5. File paths only (no line numbers until full-text search)\n\n**Simplifications Applied**:\n1. ✅ Removed multi-collection merging/ranking (replaced with collection-relevance discovery concept)\n2. ✅ Removed asyncio complexity\n3. ✅ Removed score normalization\n4. ✅ Removed merge strategies\n5. ✅ Removed line number tracking (deferred to full-text search RDR)\n\n**Time Savings**: 20.5 hours (was 28 hours originally) - 27% reduction\n\n**Future Enhancements**:\n- Collection Relevance Discovery: Find which collections have relevant content, then search targeted collection\n- Full-Text Search: Add line numbers when exact string matching is implemented\n- Hybrid Search: Combine semantic + lexical\n\n**Key Components (Final)**:\n1. Query Embedding Pipeline - Auto-detect model, cache with @lru_cache\n2. Metadata Filter Parser - Simple DSL + JSON format\n3. Single Collection Search - Synchronous, straightforward\n4. Result Formatter - File paths + content snippets (no line calc)\n\n**Result Format**:\n- Source code: `/path/to/file.py` (no line number)\n- PDFs: `/path/to/file.pdf:page12` (page useful)\n- Content snippet shows matched content\n\n**CLI Interface**:\n```bash\narcaneum search \"query\" --collection MyCode --filter language=python --limit 10\n```\n\nImplementation: 9 steps, 20.5 hours, maximum simplicity achieved","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.719691-07:00","updated_at":"2025-10-20T12:37:34.318763-07:00","closed_at":"2025-10-20T11:52:07.509124-07:00","external_ref":"doc/rdr/RDR-007-semantic-search.md","dependencies":[{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-1","type":"blocks","created_at":"2025-10-20T07:41:10.736893-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-2","type":"blocks","created_at":"2025-10-20T07:41:10.738007-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.738969-07:00","created_by":"auto-import"},{"issue_id":"arcaneum-7","depends_on_id":"arcaneum-6","type":"blocks","created_at":"2025-10-20T07:41:10.739715-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-70","title":"RDR for git-aware source code indexing to full-text search","description":"Create an RDR for indexing source code to full-text search engine with git and branch awareness, parallel to RDR-005.\n\nMust address:\n- Git project discovery (reuse RDR-005 discovery logic)\n- Full-text indexing without AST chunking (whole files or line-based?)\n- Git metadata storage: project_name, branch, commit_hash, file_path\n- Multi-branch support in full-text index\n- Change detection via git commit hashes (like RDR-005)\n- .gitignore respect (reuse RDR-005 patterns)\n- Line number tracking for exact match results\n\nKey Design Questions:\n- Index whole files or split by lines/functions?\n- How to support multi-branch (same file, different branches)?\n- Metadata schema: git_project_name, git_branch, file_path, line_number\n- Change detection: Same commit hash strategy as RDR-005?\n- Search granularity: File level or line level?\n- Integration: Dual indexing (vector + full-text) in one pass?\n\nParallel to RDR-005, enables exact string/regex search with line numbers.\n\nDepends on:\n- RDR-005 (git discovery, branch handling, change detection)\n- Full-text server setup RDR\n- Dual collection creation RDR\n\nDeliverable: Source code full-text indexing with git/branch awareness","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-20T12:56:23.0206-07:00","updated_at":"2025-10-20T12:56:23.0206-07:00"}
{"id":"arcaneum-71","title":"RDR for Claude Code integration of full-text search (complementary to vector search)","description":"Create an RDR for exposing full-text search to Claude Code, complementary to RDR-007 (semantic search).\n\nMust address:\n- CLI command for full-text search (parallel to semantic search)\n- Search syntax: exact phrases, regex, wildcards\n- Result format with line numbers (file.py:123)\n- Slash command integration following RDR-006 pattern\n- Complementary workflow to vector search (NOT merged)\n\nKey Design Questions:\n- Separate slash command: /search-text vs /search?\n- How to present choice to user (semantic vs exact)?\n- Result format: file:line (exact location needed)\n- CLI interface: arcaneum search-text \"exact phrase\" --collection MyCode\n- Integration pattern: Should mirror RDR-007 structure?\n- When to use which: Guide for Claude/users?\n\nComplementary to RDR-007, not a replacement or merger.\n\nExpected workflow:\n1. User: \"Find authentication patterns\" → Claude uses semantic search (RDR-007)\n2. User: \"Find exact string 'def authenticate'\" → Claude uses full-text search (this RDR)\n\nDepends on:\n- RDR-007 (semantic search pattern to mirror)\n- RDR-006 (Claude Code integration pattern)\n- Full-text server setup RDR\n- Full-text indexing RDRs (PDF + source code)\n\nDeliverable: Full-text search CLI + slash command, complementary to vector search","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-20T12:56:23.17888-07:00","updated_at":"2025-10-20T12:56:23.17888-07:00"}
{"id":"arcaneum-8","title":"Research embedding model flexibility and dynamic caching","description":"Investigate how to support multiple embedding models dynamically with caching. Research FastEmbed, sentence-transformers, and other libraries for on-demand model downloading and caching strategies.","notes":"Research completed. FastEmbed recommended for lightweight, self-contained CLI with automatic caching. Supports stella, bge-large, jina-code models with 1024/768 dimensions.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.720456-07:00","updated_at":"2025-10-20T07:41:10.720456-07:00","closed_at":"2025-10-19T14:30:51.232555-07:00","dependencies":[{"issue_id":"arcaneum-8","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.740757-07:00","created_by":"auto-import"}]}
{"id":"arcaneum-9","title":"Research model token length constraints and optimal chunking","description":"Investigate training token run lengths for embedding models (stella, modernbert, bge-large, jina-code). Determine optimal chunking strategies for Qdrant based on model constraints. Compare with ChromaDB learnings.","notes":"Research completed. Token limits: stella 512-1024, modernbert 8192, bge-large 512, jina-code 8192. Chunk sizes: 460-920 tokens with 10-20% overlap. Store-specific adjustments documented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-20T07:41:10.721284-07:00","updated_at":"2025-10-20T07:41:10.721284-07:00","closed_at":"2025-10-19T14:30:51.294453-07:00","dependencies":[{"issue_id":"arcaneum-9","depends_on_id":"arcaneum-3","type":"blocks","created_at":"2025-10-20T07:41:10.741543-07:00","created_by":"auto-import"}]}
